{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"mnist_fgm\"\n",
    "EXPERIMENT_DESC = \"applying the fast gradient sign (FGM) attack to a classifier trained on MNIST\"\n",
    "QUEUE_NAME = 'tensorflow_cpu'\n",
    "QUEUE_DESC = 'Tensorflow CPU Queue'\n",
    "PLUGIN_FILES = '../task-plugins/dioptra_custom/fgm_mnist_demo/'\n",
    "MODEL_NAME = \"mnist_classifier\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:80\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n",
    ")\n",
    "\n",
    "def register_python_source_file(module_name: str, filepath: Path) -> None:\n",
    "    \"\"\"Import a source file directly.\n",
    "\n",
    "    Args:\n",
    "        module_name: The module name to associate with the imported source file.\n",
    "        filepath: The path to the source file.\n",
    "\n",
    "    Notes:\n",
    "        Adapted from the following implementation in the Python documentation:\n",
    "        https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(filepath))\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "#register_python_source_file(\"scripts\", Path(\"..\", \"scripts\", \"__init__.py\"))\n",
    "\n",
    "# Register the examples/scripts directory as a Python module\n",
    "#from scripts.utils import make_tar\n",
    "#from scripts.setup import upload_experiment, run_experiment, delete_all\n",
    "from dioptra.client import connect_json_dioptra_client, connect_response_dioptra_client, select_files_in_directory, select_one_or_more_files\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_API\") is None:\n",
    "    os.environ[\"DIOPTRA_API\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(job, job_name, quiet=False):\n",
    "    n = 0\n",
    "    while job['status'] not in ['finished', 'failed']:\n",
    "        job = client.jobs.get_by_id(job['id'])\n",
    "        time.sleep(1)\n",
    "        if not quiet:\n",
    "            clear_output(wait=True)\n",
    "            display(\"Waiting for job.\" + \".\" * (n % 3) )\n",
    "        n += 1\n",
    "    if not quiet:\n",
    "        if job['status'] == 'finished':\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Job finished. Starting {job_name} job.\")\n",
    "        else:\n",
    "            raise Exception(\"Previous job failed. Please see tensorflow-cpu logs for details.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Dioptra and setup RESTAPI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client = connect_response_dioptra_client()\n",
    "client = connect_json_dioptra_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to login to the RESTAPI to be able to perform any functions. Here we create a user if it is not created already, and login with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.users.create(\n",
    "        username='pluginuser2',\n",
    "        email='pluginuser2@dioptra.nccoe.nist.gov',\n",
    "        password='pleasemakesuretoPLUGINthecomputer'\n",
    "    )\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "\n",
    "client.auth.login(\n",
    "    username='pluginuser2',\n",
    "    password='pleasemakesuretoPLUGINthecomputer'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload all the entrypoints in the src/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from local filesystem\n",
    "logging.basicConfig(level=logging.DEBUG) # Sets the root logger level\n",
    "\n",
    "response = client.workflows.import_resources(group_id=1,\n",
    "                                             source=select_files_in_directory(\"../extra/\", recursive=True),\n",
    "                                             config_path=\"dioptra.toml\",\n",
    "                                             resolve_name_conflicts_strategy=\"overwrite\",\n",
    "                                            )\n",
    "resources = response[\"resources\"]\n",
    "\n",
    "\n",
    "\n",
    "train_ep = resources[\"entrypoints\"][\"Train\"]\n",
    "fgm_ep = resources[\"entrypoints\"][\"FGM\"]\n",
    "patch_gen_ep = resources[\"entrypoints\"][\"Patch Generation\"]\n",
    "patch_apply_ep = resources[\"entrypoints\"][\"Patch Application\"]\n",
    "predict_ep = resources[\"entrypoints\"][\"Predict\"]\n",
    "metrics_ep = resources[\"entrypoints\"][\"Metrics\"]\n",
    "defense_ep = resources[\"entrypoints\"][\"Defense\"]\n",
    "\n",
    "entrypoints = [train_ep, fgm_ep, patch_gen_ep, patch_apply_ep, predict_ep, metrics_ep, defense_ep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = client.experiments.create(group_id=1, name=EXPERIMENT_NAME, description=EXPERIMENT_DESC)\n",
    "except:\n",
    "    experiment = client.experiments.get(search=f\"name:'{EXPERIMENT_NAME}'\")[\"data\"][0]\n",
    "\n",
    "try:\n",
    "    queue = client.queues.create(group_id=1, name=QUEUE_NAME, description=QUEUE_DESC)\n",
    "except:\n",
    "    queue = client.queues.get(search=f\"name:'{QUEUE_NAME}'\")[\"data\"][0]\n",
    "\n",
    "experiment_id = experiment['id']\n",
    "queue_id = queue['id']\n",
    "\n",
    "client.experiments.entrypoints.create(experiment_id=experiment_id, entrypoint_ids=entrypoints)\n",
    "\n",
    "for entrypoint in entrypoints:\n",
    "    client.entrypoints.queues.create(entrypoint_id=entrypoint, queue_ids=[queue_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a new le_net model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id, \n",
    "    description=f\"training job for {experiment_id}\", \n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=train_ep, \n",
    "    values={\"epochs\":\"3\"}, \n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples using FGM attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, 'fgm')\n",
    "fgm_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"fgm job for {experiment_id}\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=fgm_ep,\n",
    "    values={\"model_name\": MODEL_NAME, \"model_version\": str(-1)}, # -1 means get the latest model\n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate patches based on the model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, 'patch_gen')\n",
    "patch_gen_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"patch generation job for {experiment_id}\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=patch_gen_ep,\n",
    "    values={\"model_name\": MODEL_NAME,\n",
    "     \"model_version\": str(-1), # -1 means get the latest\n",
    "     \"rotation_max\": str(180),\n",
    "     \"max_iter\": str(5000),\n",
    "     \"learning_rate\": str(5.0),\n",
    "    },\n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples by attaching generated patches to the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, 'patch_apply')\n",
    "patch_apply_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"patch generation job for {experiment_id}\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=patch_apply_ep,\n",
    "    values={\"model_name\": MODEL_NAME, \n",
    "     \"model_version\": str(-1), # -1 means get the latest model\n",
    "     \"job_id\": str(patch_gen_job['id']),# we need the patches we just generated too\n",
    "     \"patch_scale\": str(0.5),\n",
    "     \"rotation_max\": str(180),\n",
    "    }, \n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to submit infer & defend jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(experiment_id, queue_id, ep, title, prev_job_id=False, latest_model=False, args=None, prev_job=None, job_time_limit='1h'):\n",
    "    args = {} if args is None else args\n",
    "    prev_job = {} if prev_job is None else prev_job\n",
    "    if prev_job is not None:\n",
    "        wait_for_job(prev_job, title, quiet=False)\n",
    "    if prev_job_id and 'id' in prev_job.keys():\n",
    "        args['job_id'] = str(prev_job['id'])\n",
    "    if latest_model:\n",
    "        args['model_name'] = MODEL_NAME \n",
    "        args['model_version'] = str(-1)\n",
    "    job = client.experiments.jobs.create(\n",
    "        experiment_id=experiment_id,\n",
    "        description=f\"{title} job for {experiment_id}\",\n",
    "        queue_id=queue_id,\n",
    "        entrypoint_id=ep,\n",
    "        values=args,\n",
    "        timeout=job_time_limit\n",
    "    )\n",
    "    return job\n",
    "def get_prev_tar_file(adv=\"def\"):\n",
    "    dd = \"def_testing\" if adv == \"def\" else \"adv_testing\"\n",
    "    tn = f\"testing_adversarial_{adv}.tar.gz\"\n",
    "    return dd, tn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defend(experiment_id, queue_id, defense_ep, prev_job, defense=\"spatial_smoothing\", adv=\"adv\", defense_kwargs=None, job_time_limit='1h'):\n",
    "    defense_kwargs = {} if defense_kwargs is None else defense_kwargs\n",
    "    dd, tn = get_prev_tar_file(adv)\n",
    "    arg_dict = {\n",
    "        \"def_type\":defense,\n",
    "        \"adv_tar_name\": tn,\n",
    "        \"defense_kwargs\": json.dumps(defense_kwargs)\n",
    "    }\n",
    "    defense_job = run_job(experiment_id, queue_id, defense_ep, defense + ' defense', prev_job_id=True, args=arg_dict, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return defense_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(experiment_id, queue_id, predict_ep, prev_job, job_time_limit='1h', adv=\"def\"):\n",
    "    dd, tn = get_prev_tar_file(adv)\n",
    "    arg_dict = {\n",
    "         \"tar_name\": tn,\n",
    "         \"data_dir\": dd,\n",
    "         #\"record_actual_class\": str(True), # add a column 'actual' representing the class with the highest probability\n",
    "         #\"record_target_class\": str(True)  # add a column 'target' representing the original class of the image\n",
    "    } # Note: using both actual and target above removes the need to load the original dataset during metrics calculation\n",
    "    predict_job = run_job(experiment_id, queue_id, predict_ep, \"predict\", prev_job_id=True, latest_model=True, args=arg_dict, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return predict_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(experiment_id, queue_id, measure_ep, prev_job, job_time_limit='1h'):\n",
    "    metrics_job = run_job(experiment_id, queue_id, measure_ep, \"metrics\", prev_job_id=True, args={}, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return metrics_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from uuid import UUID\n",
    "\n",
    "def get_metrics(job):\n",
    "    wait_for_job(job, 'metrics', quiet=True)\n",
    "    return client.jobs.get_metrics_by_id(job_id=job['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spatial Smoothing, JPEG Compression, Gaussian Defense against FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fgm = predict(experiment_id, queue_id, predict_ep, fgm_job, adv=\"fgm\")\n",
    "measure_fgm = measure(experiment_id, queue_id, metrics_ep, predict_fgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, defense=\"spatial_smoothing\", adv=\"fgm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_spatial_fgm = predict(experiment_id, queue_id, predict_ep, spatial_job_fgm, adv=\"def\")\n",
    "measure_spatial_fgm = measure(experiment_id, queue_id, metrics_ep, predict_spatial_fgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_comp_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, defense=\"jpeg_compression\", adv=\"fgm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_jpeg_comp_fgm = predict(experiment_id, queue_id, predict_ep, jpeg_comp_job_fgm, adv=\"def\")\n",
    "measure_jpeg_comp_fgm = measure(experiment_id, queue_id, metrics_ep, predict_jpeg_comp_fgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, defense=\"gaussian_augmentation\", adv=\"fgm\", defense_kwargs={\n",
    "            \"augmentation\": False,\n",
    "            \"ratio\": 1,\n",
    "            \"sigma\": .1,\n",
    "            \"apply_fit\": False,\n",
    "            \"apply_predict\": True\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gaussian_fgm = predict(experiment_id, queue_id, predict_ep, gaussian_job_fgm, adv=\"def\")\n",
    "measure_gaussian_fgm = measure(experiment_id, queue_id, metrics_ep, predict_gaussian_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spatial Smoothing, JPEG Compression, Gaussian Defense against Patch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_patch = predict(experiment_id, queue_id, predict_ep, patch_apply_job, adv=\"patch\")\n",
    "measure_patch = measure(experiment_id, queue_id, metrics_ep, predict_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, defense=\"spatial_smoothing\", adv=\"patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_spatial_patch = predict(experiment_id, queue_id, predict_ep, spatial_job_patch, adv=\"def\")\n",
    "measure_spatial_patch = measure(experiment_id, queue_id, metrics_ep, predict_spatial_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_comp_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, defense=\"jpeg_compression\", adv=\"patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_jpeg_comp_patch = predict(experiment_id, queue_id, predict_ep, jpeg_comp_job_patch, adv=\"def\")\n",
    "measure_jpeg_comp_patch = measure(experiment_id, queue_id, metrics_ep, predict_jpeg_comp_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, defense=\"gaussian_augmentation\", adv=\"patch\", defense_kwargs={\n",
    "            \"augmentation\": False,\n",
    "            \"ratio\": 1,\n",
    "            \"sigma\": .1,\n",
    "            \"apply_fit\": False,\n",
    "            \"apply_predict\": True\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gaussian_patch = predict(experiment_id, queue_id, predict_ep, gaussian_job_patch, adv=\"def\")\n",
    "measure_gaussian_patch = measure(experiment_id, queue_id, metrics_ep, predict_gaussian_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "metrics = {\n",
    "    \"trained\": get_metrics(training_job),\n",
    "    \"fgm\": get_metrics(measure_fgm),\n",
    "    \"patch\": get_metrics(measure_patch),\n",
    "    \"jpeg_fgm\": get_metrics(measure_jpeg_comp_fgm),\n",
    "    \"spatial_fgm\": get_metrics(measure_spatial_fgm),\n",
    "    \"gaussian_fgm\": get_metrics(measure_gaussian_fgm),\n",
    "    \"jpeg_patch\": get_metrics(measure_jpeg_comp_patch),\n",
    "    \"spatial_patch\": get_metrics(measure_spatial_patch),\n",
    "    \"gaussian_patch\": get_metrics(measure_gaussian_patch)\n",
    "}\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "scenarios = {\n",
    "    'trained': 'Base Model',\n",
    "    'fgm': 'Fast Gradient Method (Attack)',\n",
    "    'jpeg_fgm': 'JPEG Compression vs. FGM (Defense)',\n",
    "    'spatial_fgm': 'Spatial Smoothing vs. FGM (Defense)',\n",
    "    'gaussian_fgm': 'Gaussian Noise vs. FGM (Defense)',\n",
    "    'patch': 'Adversarial Patch (Attack)',\n",
    "    'jpeg_patch': 'JPEG Compression vs. Patch (Defense)',\n",
    "    'spatial_patch': 'Spatial Smoothing vs. Patch (Defense)',\n",
    "    'gaussian_patch': 'Gaussian Noise vs. Patch (Defense)'\n",
    "}\n",
    "names = [scenarios[k] for k in scenarios.keys()]\n",
    "values = [[job_metric['value'] * 100 for job_metric in metrics[k] if job_metric['name'] == 'accuracy'][0] for k in scenarios.keys()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "\n",
    "# Horizontal Bar Plot\n",
    "ax.barh(names, values)\n",
    "\n",
    "# Add padding between axes and labels\n",
    "ax.xaxis.set_tick_params(pad = 5)\n",
    "ax.yaxis.set_tick_params(pad = 10)\n",
    "\n",
    "# Show top values \n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add annotation to bars\n",
    "for i in ax.patches:\n",
    "    plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "             str(round((i.get_width()), 2)),\n",
    "             fontsize = 10, fontweight ='bold',\n",
    "             color ='grey')\n",
    "\n",
    "# Add Plot Title\n",
    "ax.set_title('Inference Percent Accuracy',\n",
    "             loc ='left', )\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
