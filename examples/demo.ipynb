{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"mnist_fgm\"\n",
    "EXPERIMENT_DESC = \"applying the fast gradient sign (FGM) attack to a classifier trained on MNIST\"\n",
    "QUEUE_NAME = 'Tensorflow CPU'\n",
    "QUEUE_DESC = 'Tensorflow CPU Queue'\n",
    "MODEL_NAME = \"mnist_classifier\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n",
    ")\n",
    "\n",
    "from dioptra.client import connect_json_dioptra_client, connect_response_dioptra_client, select_files_in_directory, select_one_or_more_files\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_API\") is None:\n",
    "    os.environ[\"DIOPTRA_API\"] = RESTAPI_ADDRESS\n",
    "\n",
    "def wait_for_job(job, job_name, quiet=False):\n",
    "    n = 0\n",
    "    while job['status'] not in ['finished', 'failed']:\n",
    "        job = client.jobs.get_by_id(job['id'])\n",
    "        time.sleep(1)\n",
    "        if not quiet:\n",
    "            clear_output(wait=True)\n",
    "            display(\"Waiting for job.\" + \".\" * (n % 3) )\n",
    "        n += 1\n",
    "    if not quiet:\n",
    "        if job['status'] == 'finished':\n",
    "            clear_output(wait=True)\n",
    "            display(f'Job finished. Starting \"{job_name}\" job.')\n",
    "        else:\n",
    "            raise Exception(\"Previous job failed. Please see tensorflow-cpu logs for details.\")\n",
    "    return job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Dioptra and setup RESTAPI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client = connect_response_dioptra_client()\n",
    "client = connect_json_dioptra_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to login to the RESTAPI to be able to perform any functions. Here we create a user if it is not created already, and login with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/users  method=POST\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/users HTTP/1.1\" 308 257\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/users/ HTTP/1.1\" 409 266\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=409\n",
      "DEBUG:dioptra.client.sessions:HTTP error code returned: status_code=409  method=POST  url=http://localhost:5000/api/v1/users/  text={\"error\": \"EntityExistsError\", \"message\": \"Conflict - The User with username having value (pluginuser2) is not available.\", \"detail\": {\"entity_type\": \"User\", \"existing_id\": 1, \"entity_attributes\": {\"username\": \"pluginuser2\"}}, \"originating_path\": \"/api/v1/users/?\"}\n",
      "\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/auth/login  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/auth/login HTTP/1.1\" 200 58\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'username': 'pluginuser2', 'status': 'Login successful'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    client.users.create(\n",
    "        username='pluginuser2',\n",
    "        email='pluginuser2@dioptra.nccoe.nist.gov',\n",
    "        password='pleasemakesuretoPLUGINthecomputer'\n",
    "    )\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "\n",
    "client.auth.login(\n",
    "    username='pluginuser2',\n",
    "    password='pleasemakesuretoPLUGINthecomputer'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/users  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/users HTTP/1.1\" 308 257\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/users/ HTTP/1.1\" 409 272\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=409\n",
      "DEBUG:dioptra.client.sessions:HTTP error code returned: status_code=409  method=POST  url=http://localhost:5000/api/v1/users/  text={\"error\": \"EntityExistsError\", \"message\": \"Conflict - The User with username having value (dioptra-worker) is not available.\", \"detail\": {\"entity_type\": \"User\", \"existing_id\": 2, \"entity_attributes\": {\"username\": \"dioptra-worker\"}}, \"originating_path\": \"/api/v1/users/?\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client.users.create(\n",
    "        username='dioptra-worker',\n",
    "        email='dw@dioptra.nccoe.nist.gov',\n",
    "        password='password'\n",
    "    )\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload all the entrypoints in the src/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/workflows/resourceImport  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/workflows/resourceImport HTTP/1.1\" 200 1335\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    }
   ],
   "source": [
    "# import from local filesystem\n",
    "logging.basicConfig(level=logging.DEBUG) # Sets the root logger level\n",
    "\n",
    "response = client.workflows.import_resources(group_id=1,\n",
    "                                             source=select_files_in_directory(\"../extra/\", recursive=True),\n",
    "                                             config_path=\"dioptra.toml\",\n",
    "                                             resolve_name_conflicts_strategy=\"overwrite\",\n",
    "                                            )\n",
    "resources = response[\"resources\"]\n",
    "\n",
    "train_ep = resources[\"entrypoints\"][\"Train\"]\n",
    "fgm_ep = resources[\"entrypoints\"][\"FGM\"]\n",
    "patch_gen_ep = resources[\"entrypoints\"][\"Patch Generation\"]\n",
    "patch_apply_ep = resources[\"entrypoints\"][\"Patch Application\"]\n",
    "predict_ep = resources[\"entrypoints\"][\"Predict\"]\n",
    "metrics_ep = resources[\"entrypoints\"][\"Metrics\"]\n",
    "defense_ep = resources[\"entrypoints\"][\"Defense\"]\n",
    "\n",
    "entrypoints = [train_ep, fgm_ep, patch_gen_ep, patch_apply_ep, predict_ep, metrics_ep, defense_ep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments HTTP/1.1\" 308 269\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/ HTTP/1.1\" 409 307\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=409\n",
      "DEBUG:dioptra.client.sessions:HTTP error code returned: status_code=409  method=POST  url=http://localhost:5000/api/v1/experiments/  text={\"error\": \"EntityExistsError\", \"message\": \"Conflict - The entity with name having value (mnist_fgm), and group_id having value (1) is not available.\", \"detail\": {\"entity_type\": null, \"existing_id\": 80, \"entity_attributes\": {\"name\": \"mnist_fgm\", \"group_id\": 1}}, \"originating_path\": \"/api/v1/experiments/?\"}\n",
      "\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments  method=GET\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/experiments?index=0&pageLength=10&search=name%3A%27mnist_fgm%27 HTTP/1.1\" 308 389\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/experiments/?index=0&pageLength=10&search=name%3A%27mnist_fgm%27 HTTP/1.1\" 200 666\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/queues HTTP/1.1\" 308 259\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/queues/ HTTP/1.1\" 409 312\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=409\n",
      "DEBUG:dioptra.client.sessions:HTTP error code returned: status_code=409  method=POST  url=http://localhost:5000/api/v1/queues/  text={\"error\": \"EntityExistsError\", \"message\": \"Conflict - The entity with name having value (Tensorflow CPU), and group_id having value (1) is not available.\", \"detail\": {\"entity_type\": null, \"existing_id\": 81, \"entity_attributes\": {\"name\": \"Tensorflow CPU\", \"group_id\": 1}}, \"originating_path\": \"/api/v1/queues/?\"}\n",
      "\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/queues  method=GET\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/queues?index=0&pageLength=10&search=name%3A%27Tensorflow+CPU%27 HTTP/1.1\" 308 389\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/queues/?index=0&pageLength=10&search=name%3A%27Tensorflow+CPU%27 HTTP/1.1\" 200 595\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments/80/entrypoints  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/80/entrypoints HTTP/1.1\" 200 21517\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1745/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1745/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1746/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1746/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1747/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1747/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1748/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1748/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1749/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1749/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1750/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1750/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/entrypoints/1751/queues  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/entrypoints/1751/queues HTTP/1.1\" 200 132\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    experiment = client.experiments.create(group_id=1, name=EXPERIMENT_NAME, description=EXPERIMENT_DESC)\n",
    "except:\n",
    "    experiment = client.experiments.get(search=f\"name:'{EXPERIMENT_NAME}'\")[\"data\"][0]\n",
    "\n",
    "try:\n",
    "    queue = client.queues.create(group_id=1, name=QUEUE_NAME, description=QUEUE_DESC)\n",
    "except:\n",
    "    queue = client.queues.get(search=f\"name:'{QUEUE_NAME}'\")[\"data\"][0]\n",
    "\n",
    "experiment_id = experiment['id']\n",
    "queue_id = queue['id']\n",
    "\n",
    "client.experiments.entrypoints.create(experiment_id=experiment_id, entrypoint_ids=entrypoints)\n",
    "\n",
    "for entrypoint in entrypoints:\n",
    "    client.entrypoints.queues.create(entrypoint_id=entrypoint, queue_ids=[queue_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a new le_net model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments/80/jobs  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/80/jobs HTTP/1.1\" 200 1361\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    }
   ],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id, \n",
    "    description=f\"training\", \n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=train_ep, \n",
    "    values={\"epochs\":\"3\"}, \n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples using FGM attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job finished. Starting \"fgm\" job.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments/80/jobs  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/80/jobs HTTP/1.1\" 200 1418\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    }
   ],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = wait_for_job(training_job, 'fgm')\n",
    "fgm_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"fgm\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=fgm_ep,\n",
    "    values={\"model_name\": MODEL_NAME, \"model_version\": str(-1)}, # -1 means get the latest model\n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate patches based on the model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_artifact_by_name(artifact_name, job):\n",
    "    job = wait_for_job(job, \"\", quiet=True)\n",
    "    for job_artifact in job[\"artifacts\"]:\n",
    "        artifact = client.artifacts.get_by_id(job_artifact[\"id\"])\n",
    "        print(Path(artifact[\"artifactUri\"]).name, artifact_name)\n",
    "        if Path(artifact[\"artifactUri\"]).name == artifact_name:\n",
    "            print(\"artifact:\", artifact)\n",
    "            return {\n",
    "                \"id\": artifact[\"id\"],\n",
    "                \"snapshotId\": artifact[\"snapshot\"],\n",
    "            }\n",
    "    raise Exception(\"Could not retrieve artifact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job finished. Starting \"patch_gen\" job.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments/80/jobs  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/80/jobs HTTP/1.1\" 200 1490\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    }
   ],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, 'patch_gen')\n",
    "patch_gen_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"patch generation\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=patch_gen_ep,\n",
    "    values={\n",
    "     \"model_name\": MODEL_NAME,\n",
    "     \"model_version\": str(-1), # -1 means get the latest\n",
    "     \"rotation_max\": str(180),\n",
    "     \"max_iter\": str(50),\n",
    "     \"learning_rate\": str(5.0),\n",
    "    },\n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples by attaching generated patches to the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job finished. Starting \"patch_apply\" job.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/artifacts/1753  method=GET\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/artifacts/1753 HTTP/1.1\" 200 694\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/artifacts/1753  method=GET\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/v1/artifacts/1753 HTTP/1.1\" 200 694\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n",
      "DEBUG:dioptra.client.sessions:Request made: url=http://localhost:5000/api/v1/experiments/80/jobs  method=POST\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: localhost\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"POST /api/v1/experiments/80/jobs HTTP/1.1\" 200 1542\n",
      "DEBUG:dioptra.client.sessions:Response received: status_code=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch.tar patch.tar\n",
      "artifact: {'id': 1753, 'snapshot': 2392, 'group': {'id': 1, 'name': 'public', 'url': '/api/v1/groups/1'}, 'user': {'id': 2, 'username': 'dioptra-worker', 'url': '/api/v1/users/2'}, 'createdOn': '2025-07-24T17:00:19.587465+00:00', 'snapshotCreatedOn': '2025-07-24T17:00:19.587544+00:00', 'lastModifiedOn': '2025-07-24T17:00:19.587544+00:00', 'latestSnapshot': True, 'hasDraft': False, 'tags': [], 'description': 'Artifact, patch, generated and stored as part of job, 1752', 'pluginSnapshotId': 2339, 'taskId': 537, 'isDir': False, 'fileSize': 2056, 'fileUrl': '/api/v1/artifacts/1753/contents', 'artifactUri': 'mlflow-artifacts:/0/65c1f92086564b748cfdfd7806630bf7/artifacts/patch/patch.tar', 'job': 1752}\n",
      "{'id': 1753, 'snapshotId': 2392}\n",
      "patch.tar patch.tar\n",
      "artifact: {'id': 1753, 'snapshot': 2392, 'group': {'id': 1, 'name': 'public', 'url': '/api/v1/groups/1'}, 'user': {'id': 2, 'username': 'dioptra-worker', 'url': '/api/v1/users/2'}, 'createdOn': '2025-07-24T17:00:19.587465+00:00', 'snapshotCreatedOn': '2025-07-24T17:00:19.587544+00:00', 'lastModifiedOn': '2025-07-24T17:00:19.587544+00:00', 'latestSnapshot': True, 'hasDraft': False, 'tags': [], 'description': 'Artifact, patch, generated and stored as part of job, 1752', 'pluginSnapshotId': 2339, 'taskId': 537, 'isDir': False, 'fileSize': 2056, 'fileUrl': '/api/v1/artifacts/1753/contents', 'artifactUri': 'mlflow-artifacts:/0/65c1f92086564b748cfdfd7806630bf7/artifacts/patch/patch.tar', 'job': 1752}\n"
     ]
    }
   ],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = wait_for_job(training_job, 'patch_apply')\n",
    "patch_gen_job = wait_for_job(patch_gen_job, 'patch_apply')\n",
    "print( find_artifact_by_name(\"patch.tar\", patch_gen_job))\n",
    "patch_apply_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id,\n",
    "    description=f\"patch application\",\n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=patch_apply_ep,\n",
    "    values={\n",
    "     \"model_name\": MODEL_NAME, \n",
    "     \"model_version\": str(-1), # -1 means get the latest model\n",
    "     \"patch_scale\": str(0.5),\n",
    "     \"rotation_max\": str(180),\n",
    "    }, \n",
    "    artifact_values={\n",
    "        \"patch\": find_artifact_by_name(\"patch.tar\", patch_gen_job)\n",
    "    },\n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to submit infer & defend jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(\n",
    "    experiment_id: int,\n",
    "    queue_id: int,\n",
    "    entrypoint: dict[str, any], \n",
    "    description: str,\n",
    "    previous_job: dict[str, any] = None, \n",
    "    model_name: str = None,\n",
    "    model_version: int = -1,\n",
    "    args: dict[str, any] = None, \n",
    "    artifacts: dict[str, any] = None, \n",
    "    job_time_limit: str = '1h'\n",
    "):\n",
    "    args = {} if args is None else args\n",
    "    artifacts = {} if artifacts is None else artifacts\n",
    "\n",
    "    if previous_job is not None:\n",
    "        previous_job = wait_for_job(previous_job, previous_job[\"description\"], quiet=False)\n",
    "    \n",
    "    if model_name is not None:\n",
    "        args['model_name'] = model_name \n",
    "        args['model_version'] = model_version\n",
    "    \n",
    "    job = client.experiments.jobs.create(\n",
    "        experiment_id=experiment_id,\n",
    "        description=description,\n",
    "        queue_id=queue_id,\n",
    "        entrypoint_id=ep,\n",
    "        values=args,\n",
    "        artifact_values=artifacts,\n",
    "        timeout=job_time_limit\n",
    "    )\n",
    "    \n",
    "    return job\n",
    "\n",
    "\n",
    "def defend(\n",
    "    experiment_id: int, \n",
    "    queue_id: int, \n",
    "    defense_entrypoint: dict[str, any], \n",
    "    previous_job: dict[str, any],\n",
    "    artifact_name: str,\n",
    "    defense: str = \"spatial_smoothing\", \n",
    "    defense_kwargs: dict[str, any] = None, \n",
    "    job_time_limit: str = '1h'\n",
    "):\n",
    "    defense_kwargs = {} if defense_kwargs is None else defense_kwargs\n",
    "\n",
    "    arg_dict = {\n",
    "        \"def_type\":defense,\n",
    "        \"defense_kwargs\": json.dumps(defense_kwargs)\n",
    "    }\n",
    "\n",
    "    artifacts = {\n",
    "        \"dataset\" : find_artifact_by_name(artifact_name, previous_job)\n",
    "    }\n",
    "    \n",
    "    defense_job = run_job(\n",
    "        experiment_id=experiment_id, \n",
    "        queue_id=queue_id, \n",
    "        entrypoint=defense_entrypoint, \n",
    "        description=f\"{defense} defense against {previous_job[\"\"]}\",\n",
    "        previous_job=previous_job,\n",
    "        args=arg_dict,\n",
    "        artifacts=artifacts,\n",
    "        job_time_limit=job_time_limit\n",
    "    )\n",
    "    \n",
    "    return defense_job\n",
    "\n",
    "def predict(\n",
    "    experiment_id: int,\n",
    "    queue_id: int,\n",
    "    predict_entrypoint: dict[str, any],\n",
    "    previous_job: dict[str, any],\n",
    "    artifact_name: str,\n",
    "    job_time_limit='1h'\n",
    "):\n",
    "    artifacts = {\n",
    "        \"dataset\" : find_artifact_by_name(artifact_name, previous_job)\n",
    "    }\n",
    "\n",
    "    predict_job = run_job(\n",
    "        experiment_id=experiment_id, \n",
    "        queue_id=queue_id, \n",
    "        entrypoint=predict_entrypoint,\n",
    "        description=f\"{}\",\n",
    "        model_name=MODEL_NAME,\n",
    "        args=arg_dict,\n",
    "        artifacts=artifacts,\n",
    "        previous_job=previous_job, \n",
    "        job_time_limit=job_time_limit\n",
    "    )\n",
    "\n",
    "def measure(\n",
    "    experiment_id: int, \n",
    "    queue_id: int, \n",
    "    measure_ep: dict[str, any], \n",
    "    previous_job: dict[str, any], \n",
    "    artifact_name: str = \"predictions\",\n",
    "    job_time_limit='1h'\n",
    "):\n",
    "    artifacts = {\n",
    "        \"predictions\" : find_artifact_by_name(artifact_name, previous_job)\n",
    "    }\n",
    "    \n",
    "    metrics_job = run_job(\n",
    "        experiment_id=experiment_id, \n",
    "        queue_id=queue_id, \n",
    "        entrypoint=measure_ep,\n",
    "        previous_job=previous_job,\n",
    "        args={},\n",
    "        artifacts=artifacts,\n",
    "        job_time_limit=job_time_limit\n",
    "    )\n",
    "    \n",
    "    return metrics_job\n",
    "\n",
    "def get_metrics(job):\n",
    "    wait_for_job(job, 'metrics', quiet=True)\n",
    "    return client.jobs.get_metrics_by_id(job_id=job['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Accuracy against FGM without Defenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fgm = predict(experiment_id, queue_id, predict_ep, fgm_job, artifact_name=\"adversarial_dataset\")\n",
    "measure_fgm = measure(experiment_id, queue_id, metrics_ep, predict_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spatial Smoothing against FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, artifact_name=\"adversarial_dataset\", defense=\"spatial_smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_spatial_fgm = predict(experiment_id, queue_id, predict_ep, spatial_job_fgm, artifact_name=\"defended_dataset\")\n",
    "measure_spatial_fgm = measure(experiment_id, queue_id, metrics_ep, predict_spatial_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run JPEG Compression Defense against FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_comp_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, artifact_name=\"adversarial_dataset\", defense=\"jpeg_compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_jpeg_comp_fgm = predict(experiment_id, queue_id, predict_ep, jpeg_comp_job_fgm, artifact_name=\"defended_dataset\")\n",
    "measure_jpeg_comp_fgm = measure(experiment_id, queue_id, metrics_ep, predict_jpeg_comp_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gaussian Defense against FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_job_fgm = defend(experiment_id, queue_id, defense_ep, fgm_job, artifact_name=\"adversarial_dataset\", defense=\"gaussian_augmentation\", defense_kwargs={\n",
    "        \"augmentation\": False,\n",
    "        \"ratio\": 1,\n",
    "        \"sigma\": .1,\n",
    "        \"apply_fit\": False,\n",
    "        \"apply_predict\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gaussian_fgm = predict(experiment_id, queue_id, predict_ep, gaussian_job_fgm, artifact_name=\"defended_dataset\")\n",
    "measure_gaussian_fgm = measure(experiment_id, queue_id, metrics_ep, predict_gaussian_fgm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spatial Smoothing, JPEG Compression, Gaussian Defense against Patch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_patch = predict(experiment_id, queue_id, predict_ep, patch_apply_job, artifact_name=\"adversarial_dataset\")\n",
    "measure_patch = measure(experiment_id, queue_id, metrics_ep, predict_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, artifact_name=\"adversarial_dataset\", defense=\"spatial_smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_spatial_patch = predict(experiment_id, queue_id, predict_ep, spatial_job_patch, artifact_name=\"defended_dataset\")\n",
    "measure_spatial_patch = measure(experiment_id, queue_id, metrics_ep, predict_spatial_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_comp_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, artifact_name=\"adversarial_dataset\", defense=\"jpeg_compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_jpeg_comp_patch = predict(experiment_id, queue_id, predict_ep, jpeg_comp_job_patch, artifact_name=\"defended_dataset\")\n",
    "measure_jpeg_comp_patch = measure(experiment_id, queue_id, metrics_ep, predict_jpeg_comp_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gaussian_job_patch = defend(experiment_id, queue_id, defense_ep, patch_apply_job, artifact_name=\"adversarial_dataset\", defense=\"gaussian_augmentation\", defense_kwargs={\n",
    "        \"augmentation\": False,\n",
    "        \"ratio\": 1,\n",
    "        \"sigma\": .1,\n",
    "        \"apply_fit\": False,\n",
    "        \"apply_predict\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gaussian_patch = predict(experiment_id, queue_id, predict_ep, gaussian_job_patch, artifact_name=\"defended_dataset\")\n",
    "measure_gaussian_patch = measure(experiment_id, queue_id, metrics_ep, predict_gaussian_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and Display Metrics from Dioptra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "metrics = {\n",
    "    \"trained\": get_metrics(training_job),\n",
    "    \"fgm\": get_metrics(measure_fgm),\n",
    "    \"patch\": get_metrics(measure_patch),\n",
    "    \"jpeg_fgm\": get_metrics(measure_jpeg_comp_fgm),\n",
    "    \"spatial_fgm\": get_metrics(measure_spatial_fgm),\n",
    "    \"gaussian_fgm\": get_metrics(measure_gaussian_fgm),\n",
    "    \"jpeg_patch\": get_metrics(measure_jpeg_comp_patch),\n",
    "    \"spatial_patch\": get_metrics(measure_spatial_patch),\n",
    "    \"gaussian_patch\": get_metrics(measure_gaussian_patch)\n",
    "}\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "scenarios = {\n",
    "    'trained': 'Base Model',\n",
    "    'fgm': 'Fast Gradient Method (Attack)',\n",
    "    'jpeg_fgm': 'JPEG Compression vs. FGM (Defense)',\n",
    "    'spatial_fgm': 'Spatial Smoothing vs. FGM (Defense)',\n",
    "    'gaussian_fgm': 'Gaussian Noise vs. FGM (Defense)',\n",
    "    'patch': 'Adversarial Patch (Attack)',\n",
    "    'jpeg_patch': 'JPEG Compression vs. Patch (Defense)',\n",
    "    'spatial_patch': 'Spatial Smoothing vs. Patch (Defense)',\n",
    "    'gaussian_patch': 'Gaussian Noise vs. Patch (Defense)'\n",
    "}\n",
    "names = [scenarios[k] for k in scenarios.keys()]\n",
    "values = [[job_metric['value'] * 100 for job_metric in metrics[k] if job_metric['name'] == 'accuracy'][0] for k in scenarios.keys()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "\n",
    "# Horizontal Bar Plot\n",
    "ax.barh(names, values)\n",
    "\n",
    "# Add padding between axes and labels\n",
    "ax.xaxis.set_tick_params(pad = 5)\n",
    "ax.yaxis.set_tick_params(pad = 10)\n",
    "\n",
    "# Show top values \n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add annotation to bars\n",
    "for i in ax.patches:\n",
    "    plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "             str(round((i.get_width()), 2)),\n",
    "             fontsize = 10, fontweight ='bold',\n",
    "             color ='grey')\n",
    "\n",
    "# Add Plot Title\n",
    "ax.set_title('Inference Percent Accuracy',\n",
    "             loc ='left', )\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
