{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Basic demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a lightweight demonstration of the current Dioptra demo setup with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"howard\"\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_basic\"\n",
    "\n",
    "# Address for connecting the docker container to exposed ports on the host device\n",
    "HOST_DOCKER_INTERNAL = \"host.docker.internal\"\n",
    "# HOST_DOCKER_INTERNAL = \"172.17.0.1\"\n",
    "\n",
    "# Testbed API ports\n",
    "RESTAPI_PORT = \"30080\"\n",
    "MLFLOW_TRACKING_PORT = \"35000\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = (\n",
    "    f\"http://{HOST_DOCKER_INTERNAL}:{RESTAPI_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{RESTAPI_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the AI_RESTAPI_URI variable, used to connect to RESTful API service\n",
    "os.environ[\"AI_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = (\n",
    "    f\"http://{HOST_DOCKER_INTERNAL}:{MLFLOW_TRACKING_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{MLFLOW_TRACKING_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entrypoints that we will be running in this example are implemented in the Python source files under `src/` and the `MLproject` file.\n",
    "To run these entrypoints within the testbed architecture, we need to package those files up into an archive and submit it to the Testbed RESTful API to create a new job.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file for this example, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the Testbed RESTful API using the HTTP protocol.\n",
    "We connect using the client below, which uses the environment variable `AI_RESTAPI_URI` to figure out how to connect to the Testbed RESTful API,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = utils.SecuringAIClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to register the name of the queue that is being watched for our jobs.\n",
    "The code below checks if the relevant queue named `\"tensorflow_cpu\"` exists.\n",
    "If it does, then it just returns info about the queue, if it doesn't, it then registers the new queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_queue = restapi_client.get_queue_by_name(name=\"tensorflow_cpu\")\n",
    "\n",
    "if response_queue is None or \"Not Found\" in response_queue.get(\"message\", []):\n",
    "    response_queue = restapi_client.register_queue(name=\"tensorflow_cpu\")\n",
    "\n",
    "response_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Demo: Defining Job Parameters:\n",
    "\n",
    "Here we will submit a basic job through MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def mlflow_run_id_is_not_known(response):\n",
    "    return response[\"mlflowRunId\"] is None and response[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Submit baseline job:          \n",
    "basic_job = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"hello_world\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Basic job submitted.\")\n",
    "print(\"\")\n",
    "pprint.pprint(basic_job)\n",
    "\n",
    "# Retrieve mlflow run_id\n",
    "while mlflow_run_id_is_not_known(basic_job):\n",
    "    time.sleep(1)\n",
    "    basic_job = restapi_client.get_job_by_id(basic_job[\"jobId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the job to view its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we can see the baseline output from the job:\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "basic_job_query  = mlflow_client.get_run(basic_job[\"mlflowRunId\"])\n",
    "\n",
    "pprint.pprint(basic_job_query.data.params)\n",
    "pprint.pprint(basic_job_query.data.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize job parameters, add `\"-P job_property=<job_value>\"` to the `entry_point_kwargs` field in the job submission script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit baseline job:          \n",
    "basic_job = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"hello_world\",\n",
    "    entry_point_kwargs=' '.join([\n",
    "        '-P output_log_string=\"Hello_again!\"'\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Basic job submitted.\")\n",
    "print(\"\")\n",
    "pprint.pprint(basic_job)\n",
    "\n",
    "\n",
    "# Retrieve mlflow run_id\n",
    "while mlflow_run_id_is_not_known(basic_job):\n",
    "    time.sleep(1)\n",
    "    basic_job = restapi_client.get_job_by_id(basic_job[\"jobId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can see the baseline output from the job.\n",
    "The output has changed due to the new user parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = MlflowClient()\n",
    "basic_job_query  = mlflow_client.get_run(basic_job[\"mlflowRunId\"])\n",
    "\n",
    "pprint.pprint(basic_job_query.data.params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('securing-ai-testbed': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}