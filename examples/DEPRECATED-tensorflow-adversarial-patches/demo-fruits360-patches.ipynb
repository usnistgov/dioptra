{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Fruits360-VGG16 Patch Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the adversarial patch attack applied on the VGG16 model, as well as adversarial training defenses, and **it is assumed that you have a copy of the Fruits360 dataset and have access to a CUDA-compatible GPU (this demo cannot be run on a typical personal computer)**.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Experiment Name and Fruits360 Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def register_python_source_file(module_name: str, filepath: Path) -> None:\n",
    "    \"\"\"Import a source file directly.\n",
    "\n",
    "    Args:\n",
    "        module_name: The module name to associate with the imported source file.\n",
    "        filepath: The path to the source file.\n",
    "\n",
    "    Notes:\n",
    "        Adapted from the following implementation in the Python documentation:\n",
    "        https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(filepath))\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = \"fruits360_adversarial_patches\"\n",
    "\n",
    "# Ensure that the dataset location is properly set here.\n",
    "DATASET_DIR = \"/dioptra/data/Fruits360\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:80\"\n",
    "\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_RESTAPI_URI\") is None:\n",
    "    os.environ[\"DIOPTRA_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Register the examples/scripts directory as a Python module\n",
    "register_python_source_file(\"scripts\", Path(\"..\", \"scripts\", \"__init__.py\"))\n",
    "\n",
    "from scripts.client import DioptraClient\n",
    "from scripts.utils import make_tar\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entrypoints that we will be running in this example are implemented in the Python source files under `src/` and the `src/MLproject` file.\n",
    "To run these entrypoints within Dioptra's architecture, we need to package those files up into an archive and submit it to the Dioptra RESTful API to create a new job.\n",
    "For convenience, we provide the `make_tar` helper function defined in `examples/scripts/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tar([\"src\"], WORKFLOWS_TAR_GZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = DioptraClient()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Patches: Baseline Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train our baseline VGG16 model on the Fruits360 dataset. \n",
    "We will be submitting our jobs to the `\"tensorflow_gpu\"` queue.\n",
    "Once the experiment is finished, we will examine the accuracy results of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vgg16_train = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P batch_size=20\",\n",
    "        f\"-P register_model_name={EXPERIMENT_NAME}_vgg16\",\n",
    "        \"-P image_size=224,224,3\",\n",
    "        \"-P model_architecture=vgg16\",\n",
    "        f\"-P data_dir_training={DATASET_DIR}/training\",\n",
    "        f\"-P data_dir_testing={DATASET_DIR}/testing\",\n",
    "    ]),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    timeout=\"1h\",\n",
    ")\n",
    "\n",
    "print(\"Training job for VGG16 neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_vgg16_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper functions will recheck the job responses until the job is completed or a run ID is available. \n",
    "The run ID is needed to link dependencies between jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_run_id_is_not_known(job_response):\n",
    "    return job_response[\"mlflowRunId\"] is None and job_response[\"status\"] not in [\n",
    "        \"failed\",\n",
    "        \"finished\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_run_id(job_response):\n",
    "    while mlflow_run_id_is_not_known(job_response):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "\n",
    "    return job_response\n",
    "\n",
    "\n",
    "def wait_until_finished(job_response):\n",
    "    # First make sure job has started.\n",
    "    job_response = get_run_id(job_response)\n",
    "\n",
    "    # Next re-check job until it has stopped running.\n",
    "    while (job_response[\"status\"] not in [\"failed\", \"finished\"]):\n",
    "        time.sleep(1)\n",
    "        job_response = restapi_client.get_job_by_id(job_response[\"jobId\"])\n",
    "\n",
    "    return job_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wait for the job to complete before proceeding to next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vgg16_train = wait_until_finished(response_vgg16_train)\n",
    "print(\"Training job for VGG16 neural network\")\n",
    "pprint.pprint(response_vgg16_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking baseline VGG16 job accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job has finished running we can view the results either through the MLflow URI or by accessing the job via MLflow client.\n",
    "Here we will show the baseline accuracy results from the previous training job.\n",
    "Please see [Querying the MLFlow Tracking Service](#Querying-the-MLFlow-Tracking-Service) section for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for viewing MLflow results.\n",
    "def get_mlflow_results(job_response):\n",
    "    mlflow_client = MlflowClient()\n",
    "    job_response = wait_until_finished(job_response)\n",
    "\n",
    "    if(job_response['status']==\"failed\"):\n",
    "        return {}\n",
    "\n",
    "    run = mlflow_client.get_run(job_response[\"mlflowRunId\"])\n",
    "\n",
    "    while(len(run.data.metrics) == 0):\n",
    "        time.sleep(1)\n",
    "        run = mlflow_client.get_run(job_response[\"mlflowRunId\"])\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "results = get_mlflow_results(response_vgg16_train)\n",
    "pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and Testing Adversarial Patches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create and apply the adversarial patches over our test set and evaluate the performance of the baseline model on the adversarial patches.\n",
    "We will also apply the patches over the training set for the adversarial training defense evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Generation\n",
    "\n",
    "The following job will generate the adversarial patches. \n",
    "Feel free to adjust the input parameters to see how they impact the effectiveness of the patch attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Patches\n",
    "response_vgg16_patches = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"gen_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_vgg16\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            \"-P imagenet_preprocessing=True\",\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "            \"-P num_patch_gen_samples=20\",\n",
    "            \"-P patch_target=5\"\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_train[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch attack (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_vgg16_patches)\n",
    "print(\"\")\n",
    "\n",
    "response_vgg16_patches = get_run_id(response_vgg16_patches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying and Testing Adversarial Patches\n",
    "\n",
    "Now we will apply the adversarial patches over our test set and evaluate the performance of the baseline model on the adversarial patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Patch attack on training set.\n",
    "response_deploy_vgg16_patches_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_vgg16_patches['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_vgg16\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P imagenet_preprocessing=True\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            f\"-P data_dir={DATASET_DIR}/training\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_vgg16_patches_training)\n",
    "print(\"\")\n",
    "\n",
    "response_deploy_vgg16_patches_training = get_run_id(response_deploy_vgg16_patches_training)\n",
    "\n",
    "# Deploy Patch attack on test set.\n",
    "response_deploy_vgg16_patches_testing = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"deploy_patch\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_vgg16_patches['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_vgg16\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            \"-P imagenet_preprocessing=True\",\n",
    "            f\"-P data_dir={DATASET_DIR}/testing\",\n",
    "            \"-P patch_deployment_method=corrupt\"\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_vgg16_patches[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch deployment (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_deploy_vgg16_patches_testing)\n",
    "print(\"\")\n",
    "\n",
    "response_deploy_vgg16_patches_testing = get_run_id(response_deploy_vgg16_patches_testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Attack Evaluation: Baseline VGG16 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run an inference step to check the patch-attacked dataset with our VGG16-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check patched dataset results\n",
    "response_infer_vgg16_patch = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_deploy_vgg16_patches_testing['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_vgg16\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            \"-P imagenet_preprocessing=True\",\n",
    "            \"-P batch_size=512\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_vgg16_patches_testing[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch evaluation (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_infer_vgg16_patch)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to finish\n",
    "response_infer_vgg16_patch = wait_until_finished(response_infer_vgg16_patch)\n",
    "\n",
    "# Check on the patch evaluation results\n",
    "results = get_mlflow_results(response_infer_vgg16_patch)\n",
    "print(\"Baseline model results on adversarially patched dataset: \")\n",
    "pprint.pprint(results.data.metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the adversarial patch attack causes a noticeable decrease in the model's accuracy scores.\n",
    "\n",
    "We will now test various defenses against the patch attacked images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defense: Adversarial Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the adversarial patch demo focuses on investigating effective defenses against the attack."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Training Defense\n",
    "\n",
    "We will train a new copy of the VGG16 model on training set that contains adversarial patches.\n",
    "In doing so, the model learns to ignore the adversarial patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_patches_adv_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"train_on_Fruits360_patched\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P dataset_run_id_testing={response_deploy_vgg16_patches_testing['mlflowRunId']}\",\n",
    "            f\"-P dataset_run_id_training={response_deploy_vgg16_patches_training['mlflowRunId']}\",\n",
    "            \"-P batch_size=256\",\n",
    "            f\"-P register_model_name={EXPERIMENT_NAME}_adversarial_patch_vgg16\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            \"-P epochs=10\",\n",
    "            f\"-P data_dir_testing={DATASET_DIR}/testing\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_deploy_vgg16_patches_training[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch adversarial training (VGG16 architecture) job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_patches_adv_training)\n",
    "print(\"\")\n",
    "\n",
    "response_patches_adv_training = get_run_id(response_patches_adv_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluate_adv_training = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"infer\",\n",
    "    entry_point_kwargs=\" \".join(\n",
    "        [\n",
    "            f\"-P run_id={response_deploy_vgg16_patches_testing['mlflowRunId']}\",\n",
    "            f\"-P model_name={EXPERIMENT_NAME}_adversarial_patch_vgg16\",\n",
    "            f\"-P model_version=none\",\n",
    "            \"-P image_size=224,224,3\",\n",
    "            \"-P batch_size=256\",\n",
    "        ]\n",
    "    ),\n",
    "    queue=\"tensorflow_gpu\",\n",
    "    depends_on=response_patches_adv_training[\"jobId\"],\n",
    ")\n",
    "\n",
    "print(\"Patch evaluation job submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_evaluate_adv_training)\n",
    "print(\"\")\n",
    "\n",
    "response_patches_adv_training= wait_until_finished(response_evaluate_adv_training)\n",
    "results = get_mlflow_results(response_patches_adv_training)\n",
    "print(\"Adversarial Training Results:\")\n",
    "pprint.pprint(results.data.metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
