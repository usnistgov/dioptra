{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"mnist_fgm\"\n",
    "EXPERIMENT_DESC = \"applying the fast gradient sign (FGM) attack to a classifier trained on MNIST\"\n",
    "QUEUE_NAME = 'tensorflow_cpu'\n",
    "QUEUE_DESC = 'Tensorflow CPU Queue'\n",
    "PLUGIN_FILES = '../task-plugins/dioptra_custom/vc/'\n",
    "MODEL_NAME = \"mnist_classifier\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:20080\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "import structlog\n",
    "import yaml\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.CRITICAL),\n",
    ")\n",
    "\n",
    "def register_python_source_file(module_name: str, filepath: Path) -> None:\n",
    "    \"\"\"Import a source file directly.\n",
    "\n",
    "    Args:\n",
    "        module_name: The module name to associate with the imported source file.\n",
    "        filepath: The path to the source file.\n",
    "\n",
    "    Notes:\n",
    "        Adapted from the following implementation in the Python documentation:\n",
    "        https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(filepath))\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "register_python_source_file(\"scripts\", Path(\"..\", \"scripts\", \"__init__.py\"))\n",
    "\n",
    "# Register the examples/scripts directory as a Python module\n",
    "from scripts.client import DioptraClient\n",
    "from scripts.utils import make_tar\n",
    "\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_RESTAPI_URI\") is None:\n",
    "    os.environ[\"DIOPTRA_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = DioptraClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to login to the RESTAPI to be able to perform any functions. Here we create a user if it is not created already, and login with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.users.create('pluginuser','pluginuser@dioptra.nccoe.nist.gov','pleasemakesuretoPLUGINthecomputer','pleasemakesuretoPLUGINthecomputer')\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "client.auth.login('pluginuser','pleasemakesuretoPLUGINthecomputer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to clear all experiments, entrypoints, jobs, models, plugins, tags, and queues in the database, if a fresh start is desired. It is not currently used anywhere in this notebook, but is included for utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all():\n",
    "    for d in client.experiments.get_all(pageLength=100000)['data']:\n",
    "        client.experiments.delete_by_id(d['id'])\n",
    "    for d in client.entrypoints.get_all(pageLength=100000)['data']:\n",
    "        client.entrypoints.delete_by_id(d['id'])\n",
    "    for d in client.jobs.get_all(pageLength=100000)['data']:\n",
    "        client.jobs.delete_by_id(d['id'])\n",
    "    for d in client.models.get_all(pageLength=100000)['data']:\n",
    "        client.models.delete_by_id(d['id'])\n",
    "    for d in client.plugins.get_all(pageLength=100000)['data']:\n",
    "        try:\n",
    "            client.plugins.delete_by_id(d['id'])\n",
    "        except:\n",
    "            pass\n",
    "    for d in client.tags.get_all(pageLength=100000)['data']:\n",
    "        client.tags.delete_by_id(d['id'])\n",
    "    for d in client.pluginParameterTypes.get_all(pageLength=100000)['data']:\n",
    "        try:\n",
    "            client.pluginParameterTypes.delete_by_id(d['id'])\n",
    "        except:\n",
    "            pass\n",
    "    for d in client.queues.get_all(pageLength=100000)['data']:\n",
    "        client.queues.delete_by_id(d['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used for registering plugins located in the `../examples/task-plugins/` folder, associating them with endpoints in the ./src/ folder, and then associating those endpoints with an experiment. When `run_experiment` is called, it will create plugins based on the YML files provided, and upload any additional files in the directory specified by `PLUGIN_FILES` at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_types = ['integer', 'string', 'number', 'any', 'boolean', 'null']\n",
    "\n",
    "def create_or_get_experiment(group, name, description, entrypoints):\n",
    "    found = None\n",
    "    for exp in client.experiments.get_all(search=name,pageLength=100000)['data']:\n",
    "        if exp['name'] == name:\n",
    "            found = exp\n",
    "    if (found != None):\n",
    "        client.experiments.modify_by_id(found['id'], name, description, entrypoints)\n",
    "        return found\n",
    "    else:\n",
    "        return client.experiments.create(group, name, description, entrypoints)\n",
    "def create_or_get_entrypoints(group, name, description, taskGraph, parameters, queues, plugins):\n",
    "    found = None\n",
    "    for entrypoint in client.entrypoints.get_all(search=name,pageLength=100000)['data']:\n",
    "        if entrypoint['name'] == name:\n",
    "            found = entrypoint\n",
    "    if (found != None):\n",
    "        client.entrypoints.modify_by_id(found['id'], name, description, taskGraph, parameters, queues)\n",
    "        client.entrypoints.add_plugins_by_entrypoint_id(found['id'], plugins)\n",
    "        return found\n",
    "    else:\n",
    "        return client.entrypoints.create(group, name, description, taskGraph, parameters, queues, plugins)\n",
    "def create_or_get_plugin_type(group, name, description, structure):\n",
    "    ret = None\n",
    "    for pt in client.pluginParameterTypes.get_all(pageLength=100000)['data']:\n",
    "        if (pt['name'] == name):\n",
    "            ret = pt\n",
    "    if (ret is None):\n",
    "        ret = client.pluginParameterTypes.create(group, name, description, structure)\n",
    "    return ret\n",
    "def find_plugin_type(name, types):\n",
    "    for t in types.keys():\n",
    "        if t == name:\n",
    "            return create_or_get_plugin_type(1, name, name, types[t])['id']\n",
    "    for t in basic_types:\n",
    "        if t == name:\n",
    "            return create_or_get_plugin_type(1, name, 'primitive', {})['id']\n",
    "\n",
    "    print(\"Couldn't find type\", name, \"in types definition.\")\n",
    "\n",
    "def create_or_get_queue(group, name, description):\n",
    "    ret = None\n",
    "    for queue in client.queues.get_all(pageLength=100000)['data']:\n",
    "        if queue['name'] == name:\n",
    "            ret = queue\n",
    "    if (ret is None):\n",
    "        ret = client.queues.create(group, name, description)\n",
    "    return ret\n",
    "def plugin_to_py(plugin):\n",
    "    return '../task-plugins/' + '/'.join(plugin.split('.')[:-1]) + '.py'\n",
    "def create_inputParam_object(inputs, types):\n",
    "    ret = []\n",
    "    for inp in inputs:\n",
    "        if 'name' in inp:\n",
    "            inp_name = inp['name']\n",
    "            inp_type = inp['type']\n",
    "        else:\n",
    "            inp_name = list(inp.keys())[0]\n",
    "            inp_type = inp[inp_name]\n",
    "        if 'required' in inp:\n",
    "            inp_req = inp['required']\n",
    "        else:\n",
    "            inp_req = True\n",
    "        inp_type = find_plugin_type(inp_type, types)\n",
    "        ret += [{\n",
    "           'name': inp_name,\n",
    "           'parameterType': inp_type,\n",
    "           'required': inp_req\n",
    "        }]\n",
    "    return ret\n",
    "def create_outputParam_object(outputs, types):\n",
    "    ret = []\n",
    "    for outp in outputs:\n",
    "        if isinstance(outp, dict):\n",
    "            outp_name = list(outp.keys())[0]\n",
    "            outp_type = outp[outp_name]\n",
    "        else:\n",
    "            outp_name = outp\n",
    "            outp_type = outputs[outp_name]\n",
    "        outp_type = find_plugin_type(outp_type, types)\n",
    "        ret += [{\n",
    "           'name': outp_name,\n",
    "           'parameterType': outp_type,\n",
    "        }]\n",
    "    return ret\n",
    "\n",
    "def read_yaml(filename):\n",
    "    with open(filename) as stream:\n",
    "        try:\n",
    "            ret = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return ret\n",
    "def register_basic_types(declared):\n",
    "    for q in basic_types:\n",
    "        type_def = create_or_get_plugin_type(1, q, 'primitive', {})\n",
    "    for q in declared:\n",
    "        type_def = create_or_get_plugin_type(1, q, 'declared', declared[q])\n",
    "def get_plugins_to_register(yaml_file, plugins_to_upload=None):\n",
    "    plugins_to_upload = {} if plugins_to_upload is None else plugins_to_upload\n",
    "    yaml = read_yaml(yaml_file)\n",
    "    task_graph = yaml['graph']\n",
    "    plugins = yaml['tasks']\n",
    "    types = yaml['types']\n",
    "    \n",
    "    register_basic_types(types)\n",
    "    tasks = []\n",
    "    for plugin in plugins:\n",
    "        name = plugin\n",
    "        definition = plugins[plugin]\n",
    "        python_file = plugin_to_py(definition['plugin'])\n",
    "        upload = {}\n",
    "        upload['name'] = name\n",
    "        if 'inputs' in definition:\n",
    "            inputs = definition['inputs']\n",
    "            upload['inputParams'] = create_inputParam_object(inputs, types)\n",
    "        else:\n",
    "            upload['inputParams'] = []\n",
    "        if 'outputs' in definition:\n",
    "            outputs = definition['outputs']\n",
    "            upload['outputParams'] = create_outputParam_object(outputs, types) \n",
    "        else:\n",
    "            upload['outputParams'] = []\n",
    "        if (python_file in plugins_to_upload):\n",
    "            plugins_to_upload[python_file] += [upload]\n",
    "        else:\n",
    "            plugins_to_upload[python_file] = [upload]\n",
    "    return plugins_to_upload\n",
    "def create_or_get_plugin(group, name, description):\n",
    "    ret = None\n",
    "    for plugin in client.plugins.get_all(search=name,pageLength=100000)['data']:\n",
    "        if plugin['name'] == name:\n",
    "            ret = plugin\n",
    "    if (ret is None):\n",
    "        ret = client.plugins.create(group, name, description)\n",
    "    return ret\n",
    "def create_or_modify_plugin_file(plugin_id, filename, contents, description, tasks):\n",
    "    found = None\n",
    "    for plugin_file in client.plugins.files.get_files_by_plugin_id(plugin_id, pageLength=100000)['data']:\n",
    "        if plugin_file['filename'] == filename:\n",
    "            found = plugin_file\n",
    "    if (found != None):\n",
    "        return client.plugins.files.modify_files_by_plugin_id_file_id(plugin_id, found['id'], filename, contents, description, tasks)\n",
    "    else:\n",
    "        return client.plugins.files.create_files_by_plugin_id(plugin_id, filename, contents, description, tasks)\n",
    "def register_plugins(group, plugins_to_upload):\n",
    "    plugins = []\n",
    "    for plugin_file in plugins_to_upload.keys():\n",
    "        plugin_path = Path(plugin_file)\n",
    "        contents = plugin_path.read_text().replace(\"\\r\", '')\n",
    "        tasks = plugins_to_upload[plugin_file]\n",
    "        filename = plugin_path.name\n",
    "        description = 'custom plugin for ' + filename\n",
    "        plugin_id = create_or_get_plugin(group, plugin_path.parent.name, description)['id']\n",
    "        plugins += [plugin_id]\n",
    "        uploaded_file = create_or_modify_plugin_file(plugin_id, filename, contents, description, tasks)\n",
    "    return list(set(plugins))\n",
    "def create_parameters_object(params, modify):\n",
    "    ret = []\n",
    "    type_map = {'int': 'float', 'float':'float', 'string':'string'}\n",
    "    for p in params:\n",
    "        if (type(params[p]).__name__ in type_map.keys()):\n",
    "            paramType = type_map[type(params[p]).__name__]\n",
    "            paramType='string' # TODO: remove if backend can handle types correctly\n",
    "            defaultValue = str(params[p])\n",
    "        else:\n",
    "            defaultValue = str(params[p])\n",
    "            paramType = 'string'\n",
    "\n",
    "        if p in modify.keys():\n",
    "            defaultValue = str(modify[p])\n",
    "        name = p\n",
    "        param_obj = {\n",
    "            'name': name,\n",
    "            'defaultValue': str(defaultValue),\n",
    "            'parameterType': paramType\n",
    "        }\n",
    "        ret += [param_obj]\n",
    "    return ret\n",
    "def get_graph_for_upload(yaml_text):\n",
    "    i = 0\n",
    "    for line in yaml_text:\n",
    "        if line.startswith(\"graph:\"):\n",
    "            break\n",
    "        i += 1\n",
    "    return ''.join(yaml_text[i+1:])\n",
    "def get_parameters_for_upload(yaml_text):\n",
    "    i = 0\n",
    "    for line in yaml_text:\n",
    "        if line.startswith(\"parameters:\"):\n",
    "            start = i\n",
    "        if line.startswith(\"tasks:\"):\n",
    "            break\n",
    "        i += 1\n",
    "    return yaml_text[start:i+1]\n",
    "def register_entrypoint(group, name, description, queues, plugins, yaml_file, modify_params=None):\n",
    "    modify_params = {} if modify_params is None else modify_params\n",
    "    yaml = read_yaml(yaml_file)\n",
    "    #task_graph = yaml['graph']\n",
    "    parameters = yaml['parameters']\n",
    "    \n",
    "    with open(yaml_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    task_graph = get_graph_for_upload(lines).replace('\\r','')\n",
    "    \n",
    "    entrypoint = create_or_get_entrypoints(1, name, description, task_graph, create_parameters_object(parameters, modify_params), queues, plugins)\n",
    "    return entrypoint\n",
    "def add_missing_plugin_files(location, upload):\n",
    "    p = Path(location)\n",
    "    for child in p.iterdir():\n",
    "        if (child.name.endswith('.py')):\n",
    "            if (str(child) not in upload.keys()):\n",
    "                upload[str(child)] = []\n",
    "    return upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_experiment` uses the helper functions above to do the following tasks:\n",
    "    - create a queue specified by `QUEUE_NAME` if needed\n",
    "    - upload the plugins used by the specified `entrypoint` \n",
    "    - upload any other plugin files in the directory `PLUGIN_FILES`\n",
    "    - register the entrypoint in Dioptra\n",
    "    - create the experiment (if needed) and associate the entrypoint with the experiment\n",
    "    - start a job for the specified `entrypoint` on the queue `QUEUE_NAME`\n",
    "Note that any parameters passed in to `parameters` will overwrite the defaults in the specified YML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(entrypoint, entrypoint_name, entrypoint_desc, job_time_limit, parameters={}):\n",
    "    upload = get_plugins_to_register(entrypoint, {})\n",
    "    upload = add_missing_plugin_files(PLUGIN_FILES, upload)\n",
    "    queue = create_or_get_queue(1, QUEUE_NAME, QUEUE_DESC)\n",
    "    queues = [queue['id']]\n",
    "    plugins = register_plugins(1,upload)\n",
    "    entrypoint = register_entrypoint(1, entrypoint_name, entrypoint_desc, queues, plugins, entrypoint, parameters)\n",
    "    experiment = create_or_get_experiment(1, EXPERIMENT_NAME, EXPERIMENT_DESC, [entrypoint['id']])\n",
    "    return client.experiments.create_jobs_by_experiment_id(experiment['id'], entrypoint_desc, queue['id'], entrypoint['id'], {}, job_time_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wait_for_job` stalls til the previous job was finished, which is useful for jobs which depend on the output of other jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(job, job_name):\n",
    "    n = 0\n",
    "    while job['status'] != 'finished':  \n",
    "        job = client.jobs.get_by_id(job['id'])\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "        display(\"Waiting for job.\" + \".\" * (n % 3) )\n",
    "        n += 1\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Job finished. Starting {job_name} job.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to train our model. This particular entrypoint uses a LeNet-5 model.\n",
    "Depending on the specs of your computer, it can take 5-20 minutes or longer to complete.\n",
    "If you are fortunate enough to have access to a dedicated GPU, then the training time will be much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entrypoint = 'src/train.yml'\n",
    "entrypoint_name = 'train'\n",
    "entrypoint_desc = 'training a classifier on MNIST'\n",
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = run_experiment(entrypoint, \n",
    "                              entrypoint_name, \n",
    "                              entrypoint_desc,\n",
    "                              job_time_limit,\n",
    "                              {\"epochs_p\":1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a model, next we will apply the fast-gradient method (FGM) evasion attack on it to generate adversarial images.\n",
    "\n",
    "This specific workflow is an example of jobs that contain dependencies, as the metric evaluation jobs cannot start until the adversarial image generation jobs have completed, and the adversarial image generation job cannot start until the training job has completed.\n",
    "\n",
    "Note that the training_job id is needed to tell the FGM attack which model to generate examples against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint = 'src/fgm.yml'\n",
    "entrypoint_name = 'fgm'\n",
    "entrypoint_desc = 'generating examples on mnist_classifier using the fgm attack'\n",
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, entrypoint_name)\n",
    "fgm_job = run_experiment(entrypoint,\n",
    "                         entrypoint_name,\n",
    "                         entrypoint_desc,\n",
    "                         job_time_limit,\n",
    "                         {\"training_job_id\": training_job['id']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test out the results of our adversarial attack on the model we trained earlier. This will wait for the FGM job to finish, and then evaluate the model's performance on the adversarial examples. Note that we need to know both the `fgm_job` id as well as the `training_job` id, so that this entrypoint knows which run's adversarial examples to test against which model. \n",
    "\n",
    "The previous runs are all stored in Dioptra as well, so you can always go back later and retrieve examples, models, and even the code used to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrypoint = 'src/infer.yml'\n",
    "entrypoint_name = 'infer'\n",
    "entrypoint_desc = 'evaluating performance of mnist_classifier on generated fgm examples'\n",
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(fgm_job, entrypoint_name)\n",
    "infer_job = run_experiment(entrypoint, \n",
    "                           entrypoint_name,\n",
    "                           entrypoint_desc,\n",
    "                           job_time_limit,\n",
    "                           {\"fgm_job_id\": fgm_job['id'], \"training_job_id\": training_job['id']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "mlflow_client = MlflowClient()\n",
    "mlflow_runid = client.jobs.get_mlflow_run_id(infer_job['id'])['mlflowRunId'].replace('-','') # why\n",
    "print(mlflow_runid)\n",
    "mlflow_run = mlflow_client.get_run(mlflow_runid)\n",
    "pprint.pprint(mlflow_run.data.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
