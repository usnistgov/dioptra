{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def register_python_source_file(module_name: str, filepath: Path) -> None:\n",
    "    \"\"\"Import a source file directly.\n",
    "\n",
    "    Args:\n",
    "        module_name: The module name to associate with the imported source file.\n",
    "        filepath: The path to the source file.\n",
    "\n",
    "    Notes:\n",
    "        Adapted from the following implementation in the Python documentation:\n",
    "        https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(filepath))\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Experiment name\n",
    "EXPERIMENT_NAME = \"mnist\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:20080\"\n",
    "\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_RESTAPI_URI\") is None:\n",
    "    os.environ[\"DIOPTRA_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\"\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Register the examples/scripts directory as a Python module\n",
    "register_python_source_file(\"scripts\", Path(\"..\", \"scripts\", \"__init__.py\"))\n",
    "\n",
    "from scripts.client import DioptraClient\n",
    "from scripts.utils import make_tar\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = DioptraClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.users.create('pluginuser','pluginuser@dioptra.nccoe.nist.gov','pleasemakesuretoPLUGINthecomputer','pleasemakesuretoPLUGINthecomputer')\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "client.auth.login('pluginuser','pleasemakesuretoPLUGINthecomputer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment named `\"mnist\"` exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all():\n",
    "    for d in client.experiments.get_all()['data']:\n",
    "        client.experiments.delete_by_id(d['id'])\n",
    "    for d in client.entrypoints.get_all()['data']:\n",
    "         client.entrypoints.delete_by_id(d['id'])\n",
    "    for d in client.jobs.get_all()['data']:\n",
    "         client.jobs.delete_by_id(d['id'])\n",
    "    for d in client.models.get_all()['data']:\n",
    "         client.models.delete_by_id(d['id'])\n",
    "    for d in client.plugins.get_all()['data']:\n",
    "        try:\n",
    "            client.plugins.delete_by_id(d['id'])\n",
    "        except:\n",
    "            pass\n",
    "    for d in client.tags.get_all()['data']:\n",
    "         client.tags.delete_by_id(d['id'])\n",
    "    for d in client.pluginParameterTypes.get_all()['data']:\n",
    "        try:\n",
    "            client.pluginParameterTypes.delete_by_id(d['id'])\n",
    "        except:\n",
    "            print(\"failed\", d)\n",
    "    for d in client.queues.get_all()['data']:\n",
    "        client.queues.delete_by_id(d['id'])\n",
    "delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import structlog\n",
    "\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n",
    ")\n",
    "\n",
    "import yaml\n",
    "basic_types = ['integer', 'string', 'number', 'any', 'boolean', 'null']\n",
    "def create_or_get_experiment(group, name, description, entrypoints):\n",
    "    try:\n",
    "        return client.experiments.create(group, name, description, entrypoints)\n",
    "    except:\n",
    "        found = None\n",
    "        for exp in client.experiments.get_all(search=name,pageLength=500)['data']:\n",
    "            if exp['name'] == name:\n",
    "                found = exp\n",
    "        if (found != None):\n",
    "            client.experiments.modify_by_id(found['id'], name, description, entrypoints)\n",
    "            return found\n",
    "        else:\n",
    "            print(\"Error - could not upload experiment\")\n",
    "def create_or_get_entrypoints(group, name, description, taskGraph, parameters, queues, plugins):\n",
    "    print(taskGraph)\n",
    "    try:\n",
    "        return client.entrypoints.create(group, name, description, taskGraph, parameters, queues, plugins)\n",
    "    except:\n",
    "        found = None\n",
    "        for entrypoint in client.entrypoints.get_all(search=name,pageLength=500)['data']:\n",
    "            if entrypoint['name'] == name:\n",
    "                found = entrypoint\n",
    "        if (found != None):\n",
    "            client.entrypoints.modify_by_id(found['id'], name, description, taskGraph, parameters, queues)\n",
    "            client.entrypoints.add_plugins_by_entrypoint_id(found['id'], plugins)\n",
    "            return found\n",
    "        else:\n",
    "            print(\"Error - could not upload entrypoint\")\n",
    "\n",
    "def create_or_get_plugin_type(group, name, description, structure):\n",
    "    ret = None\n",
    "    for pt in client.pluginParameterTypes.get_all(pageLength=500)['data']:\n",
    "        if (pt['name'] == name):\n",
    "            ret = pt\n",
    "    if (ret is None):\n",
    "        ret = client.pluginParameterTypes.create(group, name, description, structure)\n",
    "    return ret\n",
    "def find_plugin_type(name, types):\n",
    "    for t in types.keys():\n",
    "        if t == name:\n",
    "            return create_or_get_plugin_type(1, name, name, types[t])['id']\n",
    "    for t in basic_types:\n",
    "        if t == name:\n",
    "            return create_or_get_plugin_type(1, name, 'primitive', {})['id']\n",
    "\n",
    "    print(\"Couldn't find type\", name, \"in types definition.\")\n",
    "\n",
    "def create_or_get_queue(group, name, description):\n",
    "    ret = None\n",
    "    for queue in client.queues.get_all(pageLength=500)['data']:\n",
    "        if queue['name'] == name:\n",
    "            ret = queue\n",
    "    if (ret is None):\n",
    "        ret = client.queues.create(group, name, description)\n",
    "    return ret\n",
    "def plugin_to_py(plugin):\n",
    "    return '../task-plugins/' + '/'.join(plugin.split('.')[:-1]) + '.py'\n",
    "def create_inputParam_object(inputs, types):\n",
    "    ret = []\n",
    "    for inp in inputs:\n",
    "        if 'name' in inp:\n",
    "            inp_name = inp['name']\n",
    "            inp_type = inp['type']\n",
    "        else:\n",
    "            inp_name = list(inp.keys())[0]\n",
    "            inp_type = inp[inp_name]\n",
    "        if 'required' in inp:\n",
    "            inp_req = inp['required']\n",
    "        else:\n",
    "            inp_req = True\n",
    "        inp_type = find_plugin_type(inp_type, types)\n",
    "        ret += [{\n",
    "           'name': inp_name,\n",
    "           'parameterType': inp_type,\n",
    "           'required': inp_req\n",
    "        }]\n",
    "    return ret\n",
    "def create_outputParam_object(outputs, types):\n",
    "    ret = []\n",
    "    for outp in outputs:\n",
    "        if isinstance(outp, dict):\n",
    "            outp_name = list(outp.keys())[0]\n",
    "            outp_type = outp[outp_name]\n",
    "        else:\n",
    "            outp_name = outp\n",
    "            outp_type = outputs[outp_name]\n",
    "        outp_type = find_plugin_type(outp_type, types)\n",
    "        ret += [{\n",
    "           'name': outp_name,\n",
    "           'parameterType': outp_type,\n",
    "        }]\n",
    "    return ret\n",
    "\n",
    "def read_yaml(filename):\n",
    "    with open(filename) as stream:\n",
    "        try:\n",
    "            ret = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    return ret\n",
    "def register_basic_types(declared):\n",
    "    for q in basic_types:\n",
    "        type_def = create_or_get_plugin_type(1, q, 'primitive', {})\n",
    "    for q in declared:\n",
    "        type_def = create_or_get_plugin_type(1, q, 'declared', declared[q])\n",
    "def get_plugins_to_register(yaml_file, plugins_to_upload={}):\n",
    "    yaml = read_yaml(yaml_file)\n",
    "    task_graph = yaml['graph']\n",
    "    plugins = yaml['tasks']\n",
    "    types = yaml['types']\n",
    "    \n",
    "    register_basic_types(types)\n",
    "    tasks = []\n",
    "    for plugin in plugins:\n",
    "        name = plugin\n",
    "        definition = plugins[plugin]\n",
    "        python_file = plugin_to_py(definition['plugin'])\n",
    "        upload = {}\n",
    "        upload['name'] = name\n",
    "        if 'inputs' in definition:\n",
    "            inputs = definition['inputs']\n",
    "            upload['inputParams'] = create_inputParam_object(inputs, types)\n",
    "        else:\n",
    "            upload['inputParams'] = []\n",
    "        if 'outputs' in definition:\n",
    "            outputs = definition['outputs']\n",
    "            upload['outputParams'] = create_outputParam_object(outputs, types) \n",
    "        else:\n",
    "            upload['outputParams'] = []\n",
    "        if (python_file in plugins_to_upload):\n",
    "            plugins_to_upload[python_file] += [upload]\n",
    "        else:\n",
    "            plugins_to_upload[python_file] = [upload]\n",
    "    return plugins_to_upload\n",
    "def create_or_get_plugin(group, name, description):\n",
    "    ret = None\n",
    "    for plugin in client.plugins.get_all(search=name,pageLength=500)['data']:\n",
    "        if plugin['name'] == name:\n",
    "            ret = plugin\n",
    "    if (ret is None):\n",
    "        ret = client.plugins.create(group, name, description)\n",
    "    return ret\n",
    "def create_or_modify_plugin_file(plugin_id, filename, contents, description, tasks):\n",
    "    try:\n",
    "        return client.plugins.files.create_files_by_plugin_id(plugin_id, filename, contents, description, tasks)\n",
    "    except:\n",
    "        found = None\n",
    "        for plugin_file in client.plugins.files.get_files_by_plugin_id(plugin_id, pageLength=500)['data']:\n",
    "            if plugin_file['filename'] == filename:\n",
    "                found = plugin_file\n",
    "        if (found != None):\n",
    "            return client.plugins.files.modify_files_by_plugin_id_file_id(plugin_id, found['id'], filename, contents, description, tasks)\n",
    "        else:\n",
    "            print(\"Error - could not upload plugin file\")\n",
    "def register_plugins(group, plugins_to_upload):\n",
    "    plugins = []\n",
    "    print(plugins_to_upload)\n",
    "    for plugin_file in plugins_to_upload.keys():\n",
    "        plugin_path = Path(plugin_file)\n",
    "        contents = plugin_path.read_text().replace(\"\\r\", '')\n",
    "        tasks = plugins_to_upload[plugin_file]\n",
    "        filename = plugin_path.name\n",
    "        description = 'custom plugin for ' + filename\n",
    "        plugin_id = create_or_get_plugin(group, plugin_path.parent.name, description)['id']\n",
    "        #plugin_id = create_or_get_plugin(group, filename.replace('.py',''), description)['id']\n",
    "        plugins += [plugin_id]\n",
    "        uploaded_file = create_or_modify_plugin_file(plugin_id, filename, contents, description, tasks)\n",
    "    return list(set(plugins))\n",
    "def create_parameters_object(params):\n",
    "    ret = []\n",
    "    type_map = {'int': 'float', 'float':'float', 'string':'string'}\n",
    "    for p in params:\n",
    "        if (type(params[p]).__name__ in type_map.keys()):\n",
    "            paramType = type_map[type(params[p]).__name__]\n",
    "            paramType='string'\n",
    "            defaultValue = str(params[p])\n",
    "        else:\n",
    "            defaultValue = str(params[p])\n",
    "            paramType = 'string'\n",
    "        name = p\n",
    "        param_obj = {\n",
    "            'name': name,\n",
    "            'defaultValue': str(defaultValue),\n",
    "            'parameterType': paramType\n",
    "        }\n",
    "        ret += [param_obj]\n",
    "    return ret\n",
    "def get_graph_for_upload(yaml_text):\n",
    "    i = 0\n",
    "    for line in yaml_text:\n",
    "        if line.startswith(\"graph:\"):\n",
    "            break\n",
    "        i += 1\n",
    "    return ''.join(yaml_text[i+1:])\n",
    "def get_parameters_for_upload(yaml_text):\n",
    "    i = 0\n",
    "    for line in yaml_text:\n",
    "        if line.startswith(\"parameters:\"):\n",
    "            start = i\n",
    "        if line.startswith(\"tasks:\"):\n",
    "            break\n",
    "        i += 1\n",
    "    return yaml_text[start:i+1]\n",
    "\n",
    "def register_entrypoint(group, name, description, queues, plugins, yaml_file):\n",
    "    yaml = read_yaml(yaml_file)\n",
    "    #task_graph = yaml['graph']\n",
    "    parameters = yaml['parameters']\n",
    "    \n",
    "    with open(yaml_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    task_graph = get_graph_for_upload(lines).replace('\\r','')\n",
    "    \n",
    "    entrypoint = create_or_get_entrypoints(1, name, description, task_graph, create_parameters_object(parameters), queues, plugins)\n",
    "    return entrypoint\n",
    "\n",
    "upload = get_plugins_to_register('src/train.yml')\n",
    "upload['../task-plugins/dioptra_custom/vc/import_keras.py'] = []\n",
    "#upload = register_plugins('src/infer.yml', upload)\n",
    "#upload = register_plugins('src/fgm.yml', upload)\n",
    "queue = create_or_get_queue(1, 'tensorflow_cpu', 'Tensorflow CPU queue')\n",
    "queues = [queue['id']]\n",
    "plugins = register_plugins(1,upload)\n",
    "print(plugins)\n",
    "entrypoint = register_entrypoint(1, 'train', 'training entrypoint', queues, plugins, 'src/train.yml')\n",
    "experiment = create_or_get_experiment(1, EXPERIMENT_NAME, 'mnist classifier example', [entrypoint['id']])\n",
    "client.experiments.create_jobs_by_experiment_id(experiment['id'], 'mnist classifier training', queue['id'], entrypoint['id'], {}, '1h')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
