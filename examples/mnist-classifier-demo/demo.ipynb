{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST Classifier demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"mnist_fgm\"\n",
    "EXPERIMENT_DESC = \"applying the fast gradient sign (FGM) attack to a classifier trained on MNIST\"\n",
    "QUEUE_NAME = 'tensorflow_cpu'\n",
    "QUEUE_DESC = 'Tensorflow CPU Queue'\n",
    "PLUGIN_FILES = '../task-plugins/dioptra_custom/fgm_mnist_demo/'\n",
    "MODEL_NAME = \"mnist_classifier\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:20080\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n",
    ")\n",
    "\n",
    "def register_python_source_file(module_name: str, filepath: Path) -> None:\n",
    "    \"\"\"Import a source file directly.\n",
    "\n",
    "    Args:\n",
    "        module_name: The module name to associate with the imported source file.\n",
    "        filepath: The path to the source file.\n",
    "\n",
    "    Notes:\n",
    "        Adapted from the following implementation in the Python documentation:\n",
    "        https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n",
    "    \"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(filepath))\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "register_python_source_file(\"scripts\", Path(\"..\", \"scripts\", \"__init__.py\"))\n",
    "\n",
    "# Register the examples/scripts directory as a Python module\n",
    "from scripts.client import DioptraClient\n",
    "from scripts.utils import make_tar\n",
    "from scripts.setup import upload_experiment, run_experiment, delete_all\n",
    "\n",
    "# Set DIOPTRA_RESTAPI_URI variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_RESTAPI_URI\") is None:\n",
    "    os.environ[\"DIOPTRA_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_RESTAPI_URI`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = DioptraClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to login to the RESTAPI to be able to perform any functions. Here we create a user if it is not created already, and login with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.users.create('pluginuser','pluginuser@dioptra.nccoe.nist.gov','pleasemakesuretoPLUGINthecomputer','pleasemakesuretoPLUGINthecomputer')\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "client.auth.login('pluginuser','pleasemakesuretoPLUGINthecomputer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wait_for_job` stalls til the previous job was finished, which is useful for jobs which depend on the output of other jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(job, job_name, quiet=False):\n",
    "    n = 0\n",
    "    while job['status'] != 'finished':  \n",
    "        job = client.jobs.get_by_id(job['id'])\n",
    "        time.sleep(1)\n",
    "        if not quiet:\n",
    "            clear_output(wait=True)\n",
    "            display(\"Waiting for job.\" + \".\" * (n % 3) )\n",
    "        n += 1\n",
    "    if not quiet:\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Job finished. Starting {job_name} job.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are just uploading all of our entrypoints and the plugins they rely on to the Dioptra server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_all(client)\n",
    "experiment_id, train_ep, queue_id = upload_experiment(client, 'src/train.yml','train','training a classifier on MNIST', PLUGIN_FILES, QUEUE_NAME, QUEUE_DESC, EXPERIMENT_NAME, EXPERIMENT_DESC)\n",
    "experiment_id, fgm_ep, queue_id = upload_experiment(client, 'src/fgm.yml','fgm','generating examples on mnist_classifier using the fgm attack', PLUGIN_FILES, QUEUE_NAME, QUEUE_DESC, EXPERIMENT_NAME, EXPERIMENT_DESC)\n",
    "experiment_id, infer_ep, queue_id = upload_experiment(client, 'src/infer.yml','infer','evaluating performance of mnist_classifier on generated fgm examples', PLUGIN_FILES, QUEUE_NAME, QUEUE_DESC, EXPERIMENT_NAME, EXPERIMENT_DESC)\n",
    "experiment_id, defense_ep, queue_id = upload_experiment(client, 'src/defense.yml','defense','generating defended dataset', PLUGIN_FILES, QUEUE_NAME, QUEUE_DESC, EXPERIMENT_NAME, EXPERIMENT_DESC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to train our model. This particular entrypoint uses a LeNet-5 model.\n",
    "Depending on the specs of your computer, it can take 5-20 minutes or longer to complete.\n",
    "If you are fortunate enough to have access to a dedicated GPU, then the training time will be much shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = client.experiments.create_jobs_by_experiment_id(\n",
    "    experiment_id, \n",
    "    f\"training job for {experiment_id}\", \n",
    "    queue_id,\n",
    "    train_ep, \n",
    "    {\"epochs\":\"1\"}, \n",
    "    job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a model, next we will apply the fast-gradient method (FGM) evasion attack on it to generate adversarial images.\n",
    "\n",
    "This specific workflow is an example of jobs that contain dependencies, as the metric evaluation jobs cannot start until the adversarial image generation jobs have completed, and the adversarial image generation job cannot start until the training job has completed.\n",
    "\n",
    "Note that the training_job id is needed to tell the FGM attack which model to generate examples against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "wait_for_job(training_job, 'fgm')\n",
    "fgm_job = client.experiments.create_jobs_by_experiment_id(\n",
    "    experiment_id,\n",
    "    f\"fgm job for {experiment_id}\",\n",
    "    queue_id,\n",
    "    fgm_ep,\n",
    "    {\"model_name\": MODEL_NAME, \"model_version\": str(-1)}, # -1 means get the latest\n",
    "    job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test out the results of our adversarial attack on the model we trained earlier. This will wait for the FGM job to finish, and then evaluate the model's performance on the adversarial examples. Note that we need to know both the `fgm_job` id as well as the `training_job` id, so that this entrypoint knows which run's adversarial examples to test against which model. \n",
    "\n",
    "The previous runs are all stored in Dioptra as well, so you can always go back later and retrieve examples, models, and even the code used to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(experiment_id, queue_id, infer_ep, prev_job, job_time_limit='1h', defense=False):\n",
    "    dd = \"def_testing\" if defense else \"adv_testing\"\n",
    "    tn = \"testing_adversarial_def.tar.gz\" if defense else \"testing_adversarial_fgm.tar.gz\"\n",
    "    wait_for_job(prev_job, 'infer', quiet=False)\n",
    "    infer_job = client.experiments.create_jobs_by_experiment_id(\n",
    "        experiment_id,\n",
    "        f\"infer job for {experiment_id}\",\n",
    "        queue_id,\n",
    "        infer_ep,\n",
    "        {\"job_id\": str(prev_job['id']),\n",
    "         \"tar_name\": tn,\n",
    "         \"data_dir\": dd,\n",
    "         \"model_name\": MODEL_NAME, \"model_version\": str(-1)}, # -1 means get the latest\n",
    "        job_time_limit\n",
    "    )\n",
    "    return infer_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from uuid import UUID\n",
    "\n",
    "def get_metrics(job):\n",
    "    wait_for_job(job, 'metrics', quiet=True)\n",
    "    mlflow_client = MlflowClient()\n",
    "    mlflow_runid = UUID(client.jobs.get_mlflow_run_id(job['id'])['mlflowRunId']).hex\n",
    "    mlflow_run = mlflow_client.get_run(mlflow_runid)\n",
    "    return mlflow_run.data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_fgm = infer(experiment_id, queue_id, infer_ep, fgm_job, defense=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id, defense_ep, queue_id = upload_experiment(client, 'src/defense.yml','defense','generating defended dataset', PLUGIN_FILES, QUEUE_NAME, QUEUE_DESC, EXPERIMENT_NAME, EXPERIMENT_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "wait_for_job(fgm_job, 'defense')\n",
    "spatial_job = client.experiments.create_jobs_by_experiment_id(\n",
    "    experiment_id,\n",
    "    f\"defense job for {experiment_id}\",\n",
    "    queue_id,\n",
    "    defense_ep,\n",
    "    {\n",
    "        \"job_id\": str(fgm_job['id']),\n",
    "        \"def_type\":\"spatial_smoothing\",\n",
    "        \"defense_kwargs\": json.dumps({})\n",
    "    }, \n",
    "    job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_spatial = infer(experiment_id, queue_id, infer_ep, spatial_job, defense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "wait_for_job(fgm_job, 'defense')\n",
    "jpeg_comp_job = client.experiments.create_jobs_by_experiment_id(\n",
    "    experiment_id,\n",
    "    f\"defense job for {experiment_id}\",\n",
    "    queue_id,\n",
    "    defense_ep,\n",
    "    {\n",
    "        \"job_id\": str(fgm_job['id']),\n",
    "        \"def_type\":\"jpeg_compression\",\n",
    "        \"defense_kwargs\": json.dumps({})\n",
    "    },\n",
    "    job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_jpeg = infer(experiment_id, queue_id, infer_ep, jpeg_comp_job, defense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "wait_for_job(fgm_job, 'defense')\n",
    "gaussian_job = client.experiments.create_jobs_by_experiment_id(\n",
    "    experiment_id,\n",
    "    f\"defense job for {experiment_id}\",\n",
    "    queue_id,\n",
    "    defense_ep,\n",
    "    {\n",
    "        \"job_id\": str(fgm_job['id']),\n",
    "        \"def_type\":\"gaussian_augmentation\",\n",
    "        \"defense_kwargs\": json.dumps({\n",
    "            \"augmentation\": False,\n",
    "            \"ratio\": 1,\n",
    "            \"sigma\": 1,\n",
    "            \"apply_fit\": False,\n",
    "            \"apply_predict\": True\n",
    "        })\n",
    "    }, \n",
    "    job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_gaussian = infer(experiment_id, queue_id, infer_ep, gaussian_job, defense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "metrics = {\n",
    "    \"trained\": get_metrics(training_job),\n",
    "    \"fgm\": get_metrics(infer_fgm),\n",
    "    \"jpeg\": get_metrics(infer_jpeg),\n",
    "    \"spatial\": get_metrics(infer_spatial),\n",
    "    \"gaussian\": get_metrics(infer_gaussian)\n",
    "}\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "pp.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "scenarios = [\n",
    "    'Base Model',\n",
    "    'Fast Gradient Method (Attack)',\n",
    "    'JPEG Compression (Defense)',\n",
    "    'Spatial Smoothing (Defense)',\n",
    "    'Gaussian Noise (Defense)'\n",
    "]\n",
    "values = [\n",
    "    metrics['trained']['accuracy'] * 100,\n",
    "    metrics['fgm']['accuracy'] * 100,\n",
    "    metrics['jpeg']['accuracy'] * 100,\n",
    "    metrics['spatial']['accuracy'] * 100,\n",
    "    metrics['gaussian']['accuracy'] * 100,\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(16, 9))\n",
    "\n",
    "# Horizontal Bar Plot\n",
    "ax.barh(scenarios, values)\n",
    "\n",
    "# Add padding between axes and labels\n",
    "ax.xaxis.set_tick_params(pad = 5)\n",
    "ax.yaxis.set_tick_params(pad = 10)\n",
    "\n",
    "# Show top values \n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add annotation to bars\n",
    "for i in ax.patches:\n",
    "    plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "             str(round((i.get_width()), 2)),\n",
    "             fontsize = 10, fontweight ='bold',\n",
    "             color ='grey')\n",
    "\n",
    "# Add Plot Title\n",
    "ax.set_title('Inference Percent Accuracy',\n",
    "             loc ='left', )\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
