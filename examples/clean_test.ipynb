{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanLab Mnist Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an end-to-end demostration of Dioptra that can be run on any modern laptop.\n",
    "Please see the [example README](README.md) for instructions on how to prepare your environment for running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the necessary Python modules and ensure the proper environment variables are set so that all the code blocks will work as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"cleanlab_exp\"\n",
    "EXPERIMENT_DESC = \"experimenting with cleanlab cleaning techniques with a classifier trained on MNIST\"\n",
    "QUEUE_NAME = 'tensorflow_cpu'\n",
    "QUEUE_DESC = 'Tensorflow CPU Queue'\n",
    "MODEL_NAME = \"mnist_classifier\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = \"http://localhost:5000\"\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:35000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "import structlog\n",
    "from pathlib import Path\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.ERROR),\n",
    ")\n",
    "\n",
    "from dioptra.client import connect_json_dioptra_client, connect_response_dioptra_client, select_files_in_directory, select_one_or_more_files\n",
    "\n",
    "# Set DIOPTRA_API variable if not defined, used to connect to RESTful API service\n",
    "if os.getenv(\"DIOPTRA_API\") is None:\n",
    "    os.environ[\"DIOPTRA_API\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Set MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(job, job_name, quiet=False):\n",
    "    n = 0\n",
    "    while job['status'] not in ['finished', 'failed']:\n",
    "        job = client.jobs.get_by_id(job['id'])\n",
    "        time.sleep(1)\n",
    "        if not quiet:\n",
    "            clear_output(wait=True)\n",
    "            display(\"Waiting for job.\" + \".\" * (n % 3) )\n",
    "        n += 1\n",
    "    if not quiet:\n",
    "        if job['status'] == 'finished':\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Job finished. Starting {job_name} job.\")\n",
    "        else:\n",
    "            raise Exception(\"Previous job failed. Please see tensorflow-cpu logs for details.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a copy of the MNIST dataset when we ran `download_data.py` script. If you have not done so already, see [How to Obtain Common Datasets](https://pages.nist.gov/dioptra/getting-started/acquiring-datasets.html).\n",
    "The training and testing images for the MNIST dataset are stored within the `/dioptra/data/Mnist` directory as PNG files that are organized into the following folder structure,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mnist\n",
    "    ├── testing\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   ├── 4\n",
    "    │   ├── 5\n",
    "    │   ├── 6\n",
    "    │   ├── 7\n",
    "    │   ├── 8\n",
    "    │   └── 9\n",
    "    └── training\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        ├── 4\n",
    "        ├── 5\n",
    "        ├── 6\n",
    "        ├── 7\n",
    "        ├── 8\n",
    "        └── 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subfolders under `training/` and `testing/` are the classification labels for the images in the dataset.\n",
    "This folder structure is a standardized way to encode the label information and many libraries can make use of it, including the Tensorflow library that we are using for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Dioptra and setup RESTAPI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `examples/scripts/client.py` file that is able to connect with the Dioptra RESTful API using the HTTP protocol.\n",
    "We connect using the client below.\n",
    "The client uses the environment variable `DIOPTRA_API`, which we configured at the top of the notebook, to figure out how to connect to the Dioptra RESTful API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client = connect_response_dioptra_client()\n",
    "client = connect_json_dioptra_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to login to the RESTAPI to be able to perform any functions. Here we create a user if it is not created already, and login with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.users.create(\n",
    "        username='user',\n",
    "        email='email',\n",
    "        password='pass'\n",
    "    )\n",
    "except:\n",
    "    pass # ignore if user exists already\n",
    "\n",
    "client.auth.login(\n",
    "    username='user',\n",
    "    password='pass'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload all the entrypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from local filesystem\n",
    "logging.basicConfig(level=logging.DEBUG) # Sets the root logger level\n",
    "\n",
    "response = client.workflows.import_resources(group_id=1,\n",
    "                                             source=select_files_in_directory(\"../extra/\", recursive=True),\n",
    "                                             config_path=\"dioptra.toml\",\n",
    "                                             resolve_name_conflicts_strategy=\"overwrite\",\n",
    "                                            )\n",
    "resources = response[\"resources\"]\n",
    "\n",
    "\n",
    "\n",
    "train_ep = resources[\"entrypoints\"][\"Train\"]\n",
    "metrics_ep = resources[\"entrypoints\"][\"Metrics\"]\n",
    "clean_ep = resources[\"entrypoints\"][\"Clean\"]\n",
    "poison_ep = resources[\"entrypoints\"][\"Poison\"]\n",
    "filter_ep = resources[\"entrypoints\"][\"Filter\"]\n",
    "train_mlflow_ep = resources[\"entrypoints\"][\"Train from MLFlow\"]\n",
    "\n",
    "entrypoints = [train_ep, metrics_ep, clean_ep, poison_ep, filter_ep, train_mlflow_ep ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = client.experiments.create(group_id=1, name=EXPERIMENT_NAME, description=EXPERIMENT_DESC)\n",
    "except:\n",
    "    experiment = client.experiments.get(search=f\"name:'{EXPERIMENT_NAME}'\")[\"data\"][0]\n",
    "\n",
    "try:\n",
    "    queue = client.queues.create(group_id=1, name=QUEUE_NAME, description=QUEUE_DESC)\n",
    "except:\n",
    "    queue = client.queues.get(search=f\"name:'{QUEUE_NAME}'\")[\"data\"][0]\n",
    "\n",
    "experiment_id = experiment['id']\n",
    "queue_id = queue['id']\n",
    "\n",
    "client.experiments.entrypoints.create(experiment_id=experiment_id, entrypoint_ids=entrypoints)\n",
    "\n",
    "for entrypoint in entrypoints:\n",
    "    client.entrypoints.queues.create(entrypoint_id=entrypoint, queue_ids=[queue_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(experiment_id, queue_id, ep, title, prev_job_id=False, latest_model=False, args=None, prev_job=None, job_time_limit='1h'):\n",
    "    args = {} if args is None else args\n",
    "    if prev_job is not None:\n",
    "        wait_for_job(prev_job, title, quiet=False)\n",
    "    if prev_job_id and 'id' in prev_job.keys():\n",
    "        args['job_id'] = str(prev_job['id'])\n",
    "    if latest_model:\n",
    "        args['model_name'] = MODEL_NAME \n",
    "        args['model_version'] = str(-1)\n",
    "    job = client.experiments.jobs.create(\n",
    "        experiment_id=experiment_id,\n",
    "        description=f\"{title} job for {experiment_id}\",\n",
    "        queue_id=queue_id,\n",
    "        entrypoint_id=ep,\n",
    "        values=args,\n",
    "        timeout=job_time_limit\n",
    "    )\n",
    "    return job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison(experiment_id, queue_id, poison_ep, percent, job_time_limit='1h'):\n",
    "    arg_dict = {\n",
    "         \"percent_poison\": percent,\n",
    "    } \n",
    "    poison_job = run_job(experiment_id, queue_id, poison_ep, \"poison\", latest_model=True, args=arg_dict, job_time_limit=job_time_limit)\n",
    "    return poison_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(experiment_id, queue_id, clean_ep, prev_job, job_time_limit='1h'):\n",
    "    clean_job = run_job(experiment_id, queue_id, clean_ep, \"clean\", prev_job_id=True, latest_model=True, args=None, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return clean_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(experiment_id, queue_id, filter_ep, prev_job, job_time_limit='1h'):\n",
    "    filter_job = run_job(experiment_id, queue_id, filter_ep, \"filter_data\", prev_job_id=True, latest_model=False, args=None, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return filter_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlflow(experiment_id, queue_id, train_mlflow_ep, data_dir, tar_name, model_name, prev_job, job_time_limit='1h'):\n",
    "    arg_dict = {\n",
    "        \"data_dir\": data_dir,\n",
    "        \"tar_name\": tar_name,\n",
    "        \"register_model_name\": model_name,\n",
    "        \"epochs\": \"3\",\n",
    "    } \n",
    "    train_job = run_job(experiment_id, queue_id, train_mlflow_ep, \"train_mlflow\", prev_job_id=True, latest_model=False, args=arg_dict, prev_job=prev_job, job_time_limit=job_time_limit)\n",
    "    return train_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Regular MNIST LeNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains a MNIST LeNet model, we also use this later for our poisoning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_time_limit = '1h'\n",
    "\n",
    "training_job = client.experiments.jobs.create(\n",
    "    experiment_id=experiment_id, \n",
    "    description=f\"training job for {experiment_id}\", \n",
    "    queue_id=queue_id,\n",
    "    entrypoint_id=train_ep, \n",
    "    values={\"epochs\":\"3\"}, \n",
    "    timeout=job_time_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training based on Cleanlab outputs after poisoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few jobs poisons a dataset and trains a model on the poisoned dataset.\n",
    "\n",
    "It then runs cleanlab's analysis on the poisoned dataset, filters out all potentially problematic data, and trains a new model on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisoning Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoning = poison(experiment_id, queue_id, poison_ep, \".99\", job_time_limit='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Poisoned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poison_model = train_mlflow(experiment_id, queue_id, train_mlflow_ep, \"poison_testing\", \"poison_testing.tar.gz\", \"poisoned_mnist_classifier\", poisoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CleanLab Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning = clean(experiment_id, queue_id, clean_ep, poisoning, job_time_limit='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering = filter_data(experiment_id, queue_id, filter_ep, cleaning, job_time_limit='1h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_model = train_mlflow(experiment_id, queue_id, train_mlflow_ep, \"cleaned_data\", \"cleaned_data.tar.gz\", \"cleaned_mnist_classifier\", filtering)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
