.. NOTICE
..
.. This software (or technical data) was produced for the U. S. Government under
.. contract SB-1341-14-CQ-0010, and is subject to the Rights in Data-General Clause
.. 52.227-14, Alt. IV (DEC 2007)
..
.. © 2021 The MITRE Corporation.

.. _deployment-guide-single-machine:

Single Machine Deployment
=========================

We recommend the use of the ``docker compose`` tool to orchestrate Testbed deployments on a single machine.
The `tool's home page <https://docs.docker.com/compose/>`__ provides an excellent summary of what it is and what it does.
For convenience, we quote the summary below,

   Compose is a tool for defining and running multi-container Docker applications.
   With Compose, you use a YAML file to configure your application’s services.
   Then, with a single command, you create and start all the services from your configuration.
   To learn more about all the features of Compose, see the `list of features <https://docs.docker.com/compose/#features>`__.

   Compose works in all environments: production, staging, development, testing, as well as CI workflows. You can learn more about each case in `Common Use Cases <https://docs.docker.com/compose/#common-use-cases>`__.

   Using Compose is basically a three-step process:

   #. Define your app's environment with a ``Dockerfile`` so it can be reproduced anywhere.

   #. Define the services that make up your app in ``docker-compose.yml`` so they can be run together in an isolated environment.

   #. Run ``docker compose up`` and the `Docker compose command <https://docs.docker.com/compose/cli-command/>`__ starts and runs your entire app.
      You can alternatively run ``docker-compose up`` using the docker-compose binary.

   A ``docker-compose.yml`` looks like this:

   .. code-block:: yaml

      version: "3.9"  # optional since v1.27.0
      services:
        web:
          build: .
          ports:
            - "5000:5000"
          volumes:
            - .:/code
            - logvolume01:/var/log
          links:
            - redis
        redis:
          image: redis
      volumes:
        logvolume01: {}

   For more information about the Compose file, see the `Compose file reference <https://docs.docker.com/compose/compose-file/>`__.

   -- `Overview of Docker Compose <https://docs.docker.com/compose/>`__

Sample Deployment
-----------------

The sample ``docker-compose.yml`` file contained in the dropdown menu below is a copy of our single-machine Testbed deployment, which was for an Nvidia DGX workstation with the following specifications,

CPU
   Intel Xeon Processor with 20 physical cores at 2.2GHz
GPU
   4 Nvidia Tesla V100 (Volta) GPUs
RAM
   12.5GB per physical core
NFS Mount
   2TB of shared storage, mount point is ``/nfs/1/datasets``
Local Storage
   2TB solid state drive
Operating System
   Ubuntu 18.04 LTS

.. dropdown:: Single Machine Deployment docker-compose.yml file

   .. include:: _docker-compose-single-machine.rst

For the rest of this guide, we will discuss how to adapt this file for use in your deployment environment.
You will need root access to the machine in order to proceed, run ``sudo -s`` to become root.

We store our deployment ``docker-compose.yml`` file at the path ``/etc/securing-ai/lab-deployment/docker-compose.yml`` and set the folder permissions so that non-root users cannot access the file.

Create Application Folders on Host
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The data generated by the Minio, MLFlow Tracking, Redis, and REST API services will be mapped to the host machine's filesystem via bind mounts.
We placed these folders for our deployment under the directory ``/var/securing-ai/lab-deployment``, which looks as follows,

.. code-block:: none

   /var/securing-ai/lab-deployment
   ├── minio
   │   ├── mlflow-tracking
   │   ├── plugins
   │   └── workflow
   ├── mlflow-tracking
   │   └── mlflow-tracking.db
   ├── redis
   │   └── appendonly.aof
   └── restapi
       └── securingai.db

For the purposes of this guide, we are going to re-use the same folder structure for our deployment.
We create them as follows,

.. code-block:: sh

   # Create folders
   mkdir -p /var/securing-ai/lab-deployment/minio
   mkdir -p /var/securing-ai/lab-deployment/mlflow-tracking
   mkdir -p /var/securing-ai/lab-deployment/redis
   mkdir -p /var/securing-ai/lab-deployment/restapi

Next, we need to set the folder permissions to match the user and group ids of the non-root user accounts used by most of these services,

.. code-block:: sh

   # Prevent access to deployment folders by non-root users
   chmod 0750 /var/securing-ai

   # Configure folder owners for compatibility with Docker images
   chown -R 39000:100 /var/securing-ai/lab-deployment/mlflow-tracking
   chown -R 39000:100 /var/securing-ai/lab-deployment/restapi
   chown -R 39000:100 /var/securing-ai/lab-deployment/minio
   chown -R 999:100 /var/securing-ai/lab-deployment/redis

   # Configure folder permissions
   chmod 0700 /var/securing-ai/lab-deployment/mlflow-tracking
   chmod 0700 /var/securing-ai/lab-deployment/restapi
   chmod 0700 /var/securing-ai/lab-deployment/minio
   chmod 0700 /var/securing-ai/lab-deployment/redis

Parameters to Set/Update
~~~~~~~~~~~~~~~~~~~~~~~~

Now that we've set up our folder structure, there are a few changes that you will need to make to the ``docker-compose.yml`` file to prepare it for your environment.

.. rubric:: Configure the dataset volume

The ``nfs-datasets`` volume will need to be updated based on the details of your deployment,

.. tabbed:: Datasets on NFS share

   The following Docker volume assumes that the NFS share is the directory ``/var/nfs/datasets`` on a device with the IP address ``192.168.1.20``.
   Update it to match the details of your NFS share.

   .. code-block:: yaml

      nfs-datasets:
        driver: local
        driver_opts:
          type: nfs
          o: addr=192.168.1.20,nfsvers=4,ro,soft,nolock,async,noatime,intr,tcp,rsize=131072,wsize=131072,actimeo=1800
          device: ":/var/nfs/datasets"

.. tabbed:: Datasets in local directory

   The following Docker volume assumes that the data is stored in the local directory ``/var/nfs/datasets``.
   Update this to match the folder where you store your datasets.

   .. code-block:: yaml

      nfs-datasets:
        driver: local
        driver_opts:
          type: none
          o: bind
          device: /var/nfs/datasets

.. rubric:: Minio Username and Password

The environment variables ``MINIO_ROOT_USER`` and ``MINIO_ROOT_PASSWORD`` set the username and password needed to access the Testbed's S3 storage.
Currently, these variables are empty in the template, so fill them in (we recommend generating a long, random password for this purpose).
After you fill in these variables, you will also need to fill in all instances of the ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` variables with the same username and password.

.. rubric:: Configure the Testbed Workers

The sample deployment in ``docker-compose.yml`` has the following Testbed Workers:

- 2 Tensorflow CPU workers
- 3 Tensorflow GPU workers
- 2 PyTorch CPU workers
- 1 PyTorch GPU worker

Workers will likely need to be removed or added to suit your environment.
A good default is to have the same number of GPU workers as there are GPUs.
The number of CPU workers is more flexible and depends on how they're used.
Our default was one CPU worker for every 5 physical cores, but this can be scaled up or down to meet your needs.

Once you've settled on the number of CPU and GPU workers you need, the next step is to allocate the GPUs.
GPU-enabled containers must have the ``runtime: nvidia`` option set, and individual GPU devices are allocated using the ``NVIDIA_VISIBLE_DEVICES`` environment variable.
The GPUs are zero-indexed, so for example, if you have 2 GPUs, then the first one can be assigned using ``NVIDIA_VISIBLE_DEVICES: 0`` and the second one using ``NVIDIA_VISIBLE_DEVICES: 1``.

To allocate the CPUs, we use the ``cpuset`` and ``cpu_shares`` options.
Our deployment used the following:

.. code-block:: yaml

   services:
     # Block 1: CPUs 0-3
     redis:
       cpuset: 0-3
       cpu_shares: 1024
     minio:
       cpuset: 0-3
       cpu_shares: 1024
     mlflow-tracking:
       cpuset: 0-3
       cpu_shares: 1024
     nginx:
       cpuset: 0-3
       cpu_shares: 1024
     restpai:
       cpuset: 0-3
       cpu_shares: 1024

     # Block 2: CPUs 10-14
     tfcpu-01:
       cpuset: 10-14
       cpu_shares: 1024
     pytorchcpu-01:
       cpuset: 10-14
       cpu_shares: 1024

     # Block 3: CPUs 15-19
     tfcpu-02:
       cpuset: 15-19
       cpu_shares: 1024
     pytorchcpu-02:
       cpuset: 15-19
       cpu_shares: 1024

     # Block 4: CPUs 4-19
     tfgpu-01:
       cpuset: 4-19
       cpu_shares: 512
     tfgpu-02:
       cpuset: 4-19
       cpu_shares: 512
     tfgpu-03:
       cpuset: 4-19
       cpu_shares: 512
     pytorchgpu-01:
       cpuset: 4-19
       cpu_shares: 512

So, as we can see, in this arrangement the GPU Workers in Block 4 have exclusive access to CPUs 4-9, but also share CPUs 10-19 with Blocks 2 and 3.
The ``cpu_shares`` parameter controls the container's CPU priority when CPU resources are in high demand and only relative differences are meaningful.
So, for example, this means that the Workers in Block 2 have twice the CPU priority that the Workers in Block 4 have when using CPUS 10-14.

Tune the CPU and GPU allocations to suit your circumstances and needs.

Initializations
~~~~~~~~~~~~~~~

.. rubric:: Initialize the REST API database

The REST API database must be initialized as follows before you start using it,

.. code-block:: sh

   # Initialize the database
   docker-compose run --rm restapi --upgrade-db

   # Teardown
   docker-compose down

.. rubric:: Create the S3 workflow bucket

The bucket used to cache the job submissions must be created before jobs can be submitted to the Testbed,

.. code-block:: sh

   # Create workflow bucket
   docker-compose run --rm --entrypoint "/bin/bash" restapi -c '/usr/local/bin/s3-mb.sh \
     --endpoint-url ${MLFLOW_S3_ENDPOINT_URL} workflow'

   # Teardown
   docker-compose down

.. rubric:: Configure and synchronize the task plugins

See the :doc:`Task Plugins Management guide <task-plugins-management>`.

Deployment
~~~~~~~~~~

With everything configured and initialized, you can deploy the Testbed,

.. code-block:: sh

   docker-compose up -d

To tear down the deployment, use

.. code-block:: sh

   docker-compose down
